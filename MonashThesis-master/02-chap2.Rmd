---
chapter: 2
knit: "bookdown::render_book"
---

# Experimental design {#ch:modesign}

Steps in the experiment are:

1. Simulate data from the different models
2. Fit a linear model to the data, extract standardized residuals and fitted values
3. Save residual plots as a fixed image size
4. Train a deep learning classifier to recognise the departures from assumptions
5. Test the model's performance and compute the accuracy

## Data simulation {#sec:simulation}

These factors are being controlled in the data simulation

- Type of relationship: linear, nonlinear or heteroskedasticity
- Sample size: randomly generated between 20-800
- Image size: fixed `150x150`

<!--
**ADD: explanations of choices** 
-->

For each group (linear, nonlinear or heteroskedasticity) 900 simulated data sets will be generated at first to facilitate model modification. (The data sets will be augmented later on for training.) The sample sizes will be 900 random numbers generated from 20 to 1500. These sample sizes will be kept the same for the three groups. We choose 20 to 1500, because when the sample size is smaller than 20, say, 10, there is hardly any patterns to see (as shown in figure 2.1). And 1500 is big enough to give a good description about the true relationship within the data. Since we make the transparency of the points to be 0.01, more points being added to the plot will just make the plot looks darker.
We set image size to be 150 $\times$ 150 pixels, because it is able to show the plots clearly and also it is reasonable small to be saved and processed easily.

<!--
Attention: plots layout!!!
-->

```{r sample10, echo=FALSE, fig.cap="Three residual plots of sample size 10 generated from different models, hard to see any systematic patterns", message=FALSE, warning=FALSE}
classic<-function(n){
  beta<-runif(1,0.5,1.5)
  x<-runif(n, -3, 3)
  y<-rnorm(n, beta*x, 1)
  
  tibble(x, y)
  model<-lm(y ~ x)
  res<-residuals(model)
  yhat<-predict(model)
  res<-(res-mean(res))/sd(res)
  yhat<-(yhat-mean(yhat))/sd(yhat)
  return(tibble(res, yhat))
}

heter<-function(n){
  lambda <- runif(1,-1,1)
  beta<-runif(1,0.5,1.5)
  a <- abs(lambda)
  b <- rnorm(1)
  c <- rnorm(1)
  
  x<-runif(n, -3, 3)
  m <- mean(x)
  
  index <- sample(c(1:3),1)
  if (index==1){
    y <- rnorm(n, beta*x, exp(x*lambda)+rexp(n))
  } else if (index==2) {
    y <- rnorm(n, beta*x, a*x^2+b*x+c+abs(c-b^2/(2*a))+rexp(n))
  } else {
    y <- rnorm(n, beta*x, dnorm(x, mean = m)+rexp(n))
  }
  
  tibble(x, y)
  model<-lm(y ~ x)
  res<-residuals(model)
  yhat<-predict(model)
  res<-(res-mean(res))/sd(res)
  yhat<-(yhat-mean(yhat))/sd(yhat)
  return(tibble(res, yhat))
}

nonpoly<-function(n){
  p1<-rnorm(1,0,2)
  p2<-rnorm(1,0,2)
  p3<-rnorm(1,0,2)*rbinom(1,1,0.5)
  p4<-rnorm(1,0,2)*rbinom(1,1,0.3)
  p5<-rnorm(1,0,1)*rbinom(1,1,0.2)
  
  x<-runif(n,-3,3)
  y<-rnorm(n, p1*x+p2*x^2+p3*x^3+p4*x^4+p5*x^5, p1^2+2*p2^2+2*p3^2+2*p4^2+2*p5^2)
  
  tibble(x, y)
  model<-lm(y ~ x)
  res<-residuals(model)
  yhat<-predict(model)
  res<-(res-mean(res))/sd(res)
  yhat<-(yhat-mean(yhat))/sd(yhat)
  return(tibble(res, yhat))
}
set.seed(0404)

c10 <- classic(10) %>% 
  ggplot(aes(yhat, res))+geom_point()

h10 <- heter(10) %>% 
  ggplot(aes(yhat, res))+geom_point()

p10 <- nonpoly(10) %>% 
  ggplot(aes(yhat, res))+geom_point()

library(gridExtra)
grid.arrange(c10, h10, p10, ncol=1)
```


### Models for relationship

#### Classic linear model 

This is the null scenario. If the data arises from a classical linear model, then the residual plots will not show departures from the assumptions. What's more, after standardization, all residuals will follow standard normal distribution. Therefore, number of explanatory variables will not make any differences to the residual plots, neither does the variance of the error term nor the correlation coefficient between X and Y. So we will set the variance of error term to be one, and randomly select a correlation from 0.5 to 1.5. However, the distributions of X does have an impact on the shape of our scatter plots. For instance, if X are generated from a uniform distribution, then the plots will look like a square especially when the sample size is large; while more like a circle if X follows normal distribution. In this paper, we are going to fix the distribution of X variables as the same normal distribution for all three groups of data. This way, we treat the X variables as non-random as what we do in practice, so we can compare different relationship of X and Y more easily.

$$y_i = \beta_0 + \beta_1 X_i + \varepsilon_i, ~~i=1, \dots , n$$

with elements of the model generated by the following processes

- $X\sim U[-3,3]$
- $\beta_0 = 0$
- $\beta_1\sim U[0.5, 1.5]$
- $\varepsilon\sim N(0, 1)$
- $n\sim U[20, 800]$

**ADD: explanation of choices, eg error variance=1**

<!--
Classic linear model is the most straightforward type to be designed. Since we will fit a linear model to the data, residual plot of this type will show no patterns in general. However, there may still be some undesired patterns formed out of randomness, especially when the sample size is small. We will simply ignore these noises and group them altogether as residuals from "classic linear model". And also, the correlation between the two original variables makes little difference on the final resiual plot, so we arbitrarily choose the coefficient from a uniform distribution from 0.5 to 1.5. Below are four examples generated from this model.
-->


```{r classic_plot, message=FALSE, cache=TRUE}
library(gridExtra)
classic<-function(n){
  
  beta<-runif(1,0.5,1.5)
  
  x<-runif(n, -3, 3)
  y<-rnorm(n, beta*x, 1)
  
  tibble(x, y)
  model<-lm(y ~ x)
  res<-residuals(model)
  yhat<-predict(model)
  res<-(res-mean(res))/sd(res)
  yhat<-(yhat-mean(yhat))/sd(yhat)
  return(tibble(res, yhat))
}
set.seed(0405)
N <- sample(20:800, 4, replace=TRUE)
  cls <- tibble(N) %>% 
    mutate(res=map(N, classic))
    cls <- cls$res
    
p1 <- ggplot(cls[[1]], aes(x=yhat, y=res), alpha=0.01) + 
  geom_point() + 
  xlab("Fitted values") + ylab("Residuals")
p2 <- ggplot(cls[[2]], aes(x=yhat, y=res), alpha=0.01) + 
  geom_point() + 
  xlab("Fitted values") + ylab("Residuals")
p3 <- ggplot(cls[[3]], aes(x=yhat, y=res), alpha=0.01) + 
  geom_point() + 
  xlab("Fitted values") + ylab("Residuals")
p4 <- ggplot(cls[[4]], aes(x=yhat, y=res), alpha=0.01) + 
  geom_point() + 
  xlab("Fitted values") + ylab("Residuals")
  
grid.arrange(p1,p2,p3,p4,nrow=2, heights=c(1,1)) 

```




#### Linear Model with Heteroskedasticity {#sec:heter}

**CHANGE THIS SECTION TO BE EXPLICIT LIKE THE CLASSIC MODEL**

The only difference of designing this model is to give a non-zero relationship between variance of y and x. It is impossible to list out all types of heteroskedasticity via a simple function form. Here I am trying to present four kinds mainly. The first two is when the variace of y is monotonically increasing or decreasing with x, while the other two has a turn point in the middle. Y will be generated from a normal distribution with mean equals to $\beta x$ and variance from one of the three equations below.

$$var(y_i)=e^{\lambda x_i}+\epsilon_i$$ where $\lambda$ is randomly chosen from a uniform distribution from -1 to 1, and $\epsilon$ is generated from an exponential distribution independently. This function is used for providing monotonic changing variance. When $\lambda$ is positive, it will give us increasing variance, and decreasing when negative. Both deterministic and random parts are from exponential family, this ensures the variance is positive.

$$var(y_i)= ax_i^2+bx_i+c+\left|c-\frac{b^2}{2a}\right|+\epsilon_i$$ where $\epsilon$ is generated from an exponential distribution independently, a is a positive random number, b and c are randomly selected from standard normal distribution. The absolute value in the middle guarantees the variance to be positive. Variance from this function decreases at first and then becomes increasing.

The third one is the density function of standard normal with x being the quantiles, and it also has an $\epsilon$ to produce some randomness. Obviously this function will make the variance increase at first and then become decrease. Below are four examples generated from this model.

```{r heter_plot, message=FALSE, cache=TRUE}
heter<-function(n, index){
  lambda <- runif(1,-2,2)
  beta<-runif(1,0.5,1.5)
  a <- abs(lambda)
  b <- rnorm(1)
  c <- rnorm(1)
  
  x<-runif(n, -3, 3)
  m <- mean(x)

  if (index==1){
    y <- rnorm(n, beta*x, exp(x*lambda)+rexp(n))
  } else if (index==2) {
    y <- rnorm(n, beta*x, a*x^2+b*x+c+abs(c-b^2/(2*a))+rexp(n))
  } else {
    y <- rnorm(n, beta*x, dnorm(x, mean = m)+rexp(n))
  }
  
  tibble(x, y)
  model<-lm(y ~ x)
  res<-residuals(model)
  yhat<-predict(model)
  res<-(res-mean(res))/sd(res)
  yhat<-(yhat-mean(yhat))/sd(yhat)
  return(tibble(res, yhat))
}
set.seed(0403)
N12 <- sample(20:800, 2, replace=TRUE)
  het12 <- tibble(N12) %>% 
    mutate(res=map(N12, heter, index=1))
    het12 <- het12$res
N3 <- sample(20:800, 1, replace=TRUE)
  het3 <- tibble(N3) %>% 
    mutate(res=map(N3, heter, index=2))
    het3 <- het3$res
N4 <- sample(20:800, 1, replace=TRUE)
  het4 <- tibble(N4) %>% 
    mutate(res=map(N4, heter, index=3))
    het4 <- het4$res
    
p1 <-   ggplot(het12[[1]], aes(x=yhat, y=res), alpha=0.01) +geom_point()
p2 <-   ggplot(het12[[2]], aes(x=yhat, y=res), alpha=0.01) +geom_point()
p3 <-   ggplot(het3[[1]], aes(x=yhat, y=res), alpha=0.01) + geom_point()
p4 <-   ggplot(het4[[1]], aes(x=yhat, y=res), alpha=0.01) + geom_point()
  
grid.arrange(p1,p2,p3,p4,nrow=2, heights=c(1,1)) 

```

#### Non-linear model {#sec:poly}

The relatively most difficult part is the non-linear relationship, given the tremendous possibilities. Luckily, we do not need to list out every single possibility. Because our purpose is for the model to tell if there is any kind of non-linear patterns, but not to specify which type it is. As long as the curve looks proper, we do not worry about if it is from a logarithm function or squared root. Therefore all we want is for the model to "see" varied patterns, we just need to find a way to show different curvature. Therotically, we can make use of the Taylor series to generate all kinds of continous and differentiable non-linear relationship. But we cannot include infinite orders of x neither can we generate infinite parameters for each order. Therefore, at this stage, I only include a polynomial of x to the order of five with each parameter being randomly generated. I chose five because normally the order of the non-linear relationship of data from Economics world do not exceed four. What's more, it is already good enough to vary the shape of our plots. Also, since order four and five are not as common as lower orders, we attach a bernoulli term with 0.5 success rate to them respectively. So the Y will be a normal distribution with mean $\beta_1x+\beta_2x^2+\beta_3x^3+\beta_4x^4+\beta_5x^5$, and variance $\beta_1^2+\beta_2^2+\beta_3^2+2\beta_4^2+2\beta_5^2$. We adjust the variance to be increase for higher orders so the data could has more variation. Below are four examples generated from this model.

```{r poly_plot, message=FALSE, cache=TRUE}
nonpoly<-function(n){
  
  p1<-rnorm(1,0,2)
  p2<-rnorm(1,0,2)
  p3<-rnorm(1,0,2)*rbinom(1,1,0.5)
  p4<-rnorm(1,0,2)*rbinom(1,1,0.3)
  p5<-rnorm(1,0,1)*rbinom(1,1,0.2)
  
  x<-runif(n,-3,3)
  y<-rnorm(n, p1*x+p2*x^2+p3*x^3+p4*x^4+p5*x^5, p1^2+2*p2^2+2*p3^2+2*p4^2+2*p5^2)
  
  tibble(x, y)
  model<-lm(y ~ x)
  res<-residuals(model)
  yhat<-predict(model)
  res<-(res-mean(res))/sd(res)
  yhat<-(yhat-mean(yhat))/sd(yhat)
  return(tibble(res, yhat))
}

set.seed(0002)
N <- sample(20:800, 4, replace=TRUE)
  pl <- tibble(N) %>% 
    mutate(res=map(N, nonpoly))
    pl <- pl$res
    
p1 <-   ggplot(pl[[1]], aes(x=yhat, y=res, alpha=0.01)) +geom_point()
p2 <-   ggplot(pl[[2]], aes(x=yhat, y=res, alpha=0.01)) +geom_point()
p3 <-   ggplot(pl[[3]], aes(x=yhat, y=res, alpha=0.01)) +geom_point()
p4 <-   ggplot(pl[[4]], aes(x=yhat, y=res, alpha=0.01)) +geom_point()
  
grid.arrange(p1,p2,p3,p4,nrow=2, heights=c(0.6,0.6)) 

```


## Extract residuals and fitted values



## Scale data into fixed image size

## Train the deep learning classifier

## Assess the accuracy



























