---
chapter: 2
knit: "bookdown::render_book"
---

# Comparing computer performance against database of human evaluation

A database of human evaluations of scatterplots of residuals against fitted is available from prior studies. This is used to compare the performance of the computer model. The computer model is trained on a broader parameter simulation framework, and tested on the same data as the human evaluations. 

## Turk study explanation

A large database of results from a human subjects was collected examine the performance of the lineup protocol relative to a classical tests. The work is published in @MM13. This database forms the basis of the test set used to examine the computer model performance. 

In @MM13, "three experiments were conducted to evaluate the effectiveness of the lineup protocol relative to the equivalent test statistic used in the regression setting." In each experiment, they simulated data from a controlled setting and then generated associated lineup for human to evaluate.

The controlled model in their first experiment is 
$$Y_i=\beta_0+\beta_1 X_{i1}+\beta_2 X_{i2}+\epsilon_i$$ where $\beta_0=5, \beta_1=15, X_1 \sim Poisson(\lambda=30), \epsilon_i\sim N(0,\sigma^2), i=1,2,...,n$. While in the null model $\beta_2=0$, and the null data was generated by simulating from $N(0,\hat{\sigma}^2)$. This experiment was aimed to test the ability of human on detecting the effect of $X_2$.

Their second experiment is very similar to the first one, but there is only one continuous variable $X_1$ on the right hand side. The actual data model is
$$Y_i=\beta_0+\beta_1 X_{i1}+\epsilon_i$$ where $\beta_0=6, X_1\sim N(0,1)$, and the null data was generated from $N(X\hat{\beta}, \hat{\sigma}^2)$.

The third experiment in their paper contains contaminated data where the actual data were in fact generated from two different specifications. 
$$Y_i=
  \begin{cases}
    \alpha+\beta X_i+\epsilon_i       & \quad X_i\sim N(0,1)\ \ i=1,...,n\\
    \lambda+\eta_i  & \quad X_i\sim N(\mu,1/3)\ \ i=1,...,n_c
  \end{cases}
$$ where $\epsilon_i \sim N(0,\sigma), \eta_i \sim N(0,\sigma/3), \ \mu=-1.75, \ \beta\in(0.1, 0.4, 0.75, 1.25, 1.5, 2.25)$. And $n=100, n_c=15, alpha=0, \lambda=10, \sigma=3.5$. The null plots were generated from $N(0,\hat{\sigma}^2)$.

Other parameters in the "actual data sets" of experiment one and two are shown in the table below. 

```{r turktable,ref.label='turktable', echo=F, results='asis'}
c1 <- c(100,100,300,300)
c2 <- c(5,12,5,12)
c3 <- c("0,1,3,5,8", "1,3,8,10,16", "0,1,2,3,5", "1,3,5,7,10")
c4 <- c("0.25, 0.75, 1.25, 1.75, 2.75", "0.5, 1.5, 3.5, 4.5, 6", "0.1, 0.4, 0.7, 1, 1.5","0, 0.8, 1.75, 2.3, 3.5")
tibble(c1,c2,c3,c4) %>% kable(col.names = c("Sample size (n)", "Error SD(sigma)", "Experiment 1 beta2", "Experiment 2 beta1"), align = "c", caption = "Combination of parameter values used for simulation in Turk's study.")
```

Their experiment 2 examined the performance of humans in recognising linear association between two variables, in direct comparison to conducting a $t$-test of $H_o: \beta_k=0$ vs $H_a: \beta_k\neq 0$ assessing the importance of including variable $k$ in the linear model. An example lineup is shown in Figure \ref{expt2}. For this lineup, 63 of the 65 people who examined it selected the data plot (position 20) from the null plots. There is clear evidence that the data displayed in plot 20 is not from $H_o: \beta_k=0$. 

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/plot_turk2_300_350_12_3.png}}
\caption{One of 70 lineups used in experiment 2 Majumder et al (2012). Of the 65 people who examined the lineup,  63 selected the data plot, which is in position 20.}
\label{expt2}
\end{figure*}

This experiment 2 utilised 70 lineups of size 20 plot, with varying degrees of departure from the $H_o: \beta_k=0$. There were 351 evaluations by human subjects. These results will be used for comparison with the deep learning model. 

## Linear relationship simulation

The design for this model is similar to what @SIM18 did in his blog but is tailored to compare the computer performance with the Turk study results.

The model is designed as:
$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$
which is the same with the data generating model of Turk's experiment 2. And all the parameters in our model will be designed to cover the range used in the second experiment in @MM13. The relevant parameters are generated using the following specification.

- $X \sim N[0,\ 1]$  
Distributions of X has impact on the shape of the scatters. For instance, if X is generated from a uniform distribution, then the plots will look like a square especially when the sample size is large; while more like a circle if X follows normal distribution. 

- $\beta_0 = 0$  
Intercept is set to be zero, because it will not change the patterns in the data plots.

- $\beta_1\sim U[-10, -0.1] \bigcup [0.1, 10]$  
$\beta_1$ is designed to be uniformly generated from -10 to 10 (excluding -0.1 to 0.1).

- $\varepsilon\sim N(0, \sigma^2) \ where\ \sigma \sim U[1,12]$  
$\varepsilon$ is designed to be uniformly generated from 1 to 12.

- $n=U[50,500]$  
The sample sizes of each data set vary from 50 to 500.

Figure \ref{fig:linear} shows four example plots generated using the specifications above. Under this controlled structure, a total number of 240,000 data sets are simulated. The histograms of the simulated $n, \beta, \sigma$, the estimated p value and the scatter plots of $\beta$ against n, $\sigma$ against n in figure \ref{fig:simplot} show good coverage over all the values.

```{r linear, fig.height=6, fig.width=6, echo=FALSE, fig.cap="Four examples of data plots generated from the classic linear model."}
library(gridExtra)
linear<-function(n){
   beta <- sample(c((-10:-0.1),(0.1:10)), 1)
  
  sigma <- sample(1:12 , 1)
  x<-rnorm(n, 0, 1)
  y<-rnorm(n, beta*x, sigma)
  
  tibble(x, y) %>% 
  ggplot(aes(x = x, y = y)) + 
    geom_point(alpha = 0.4) +
    theme(aspect.ratio = 1)
}
set.seed(0420)
n1 <- sample(c(100, 300), 1)
n2 <- sample(c(100, 300), 1)
n3 <- sample(c(100, 300), 1)
n4 <- sample(c(100, 300), 1)

p1 <- linear(n1)
p2 <- linear(n2)
p3 <- linear(n3)
p4 <- linear(n4)

grid.arrange(p1,p2,p3,p4,nrow=2, heights=c(1,1)) 

```


```{r simplot, fig.height=8, fig.width=6, fig.cap="Overview of parameter values used in the linear class simulation, for computer model training. Good coverage is obtained across the parameter space."}
parameters_b <- read.csv("data/parameters_b.csv")
hist_n_l <- ggplot(data = parameters_b, aes(n)) + geom_histogram(binwidth = 50) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated n")

hist_b_l <- ggplot(data = parameters_b, aes(beta)) + geom_histogram(binwidth = 1) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated beta from linear model")

hist_p_l <- ggplot(data = parameters_b, aes(ct.p.value)) + geom_histogram(binwidth = 0.1) +
  xlab("p-value") + 
  theme(plot.title = element_text(size = 8)) + 
  ggtitle("Histogram of estimated p value of t-test from linear model")

hist_s_l <- ggplot(data = parameters_b, aes(sigma)) + geom_histogram(binwidth = 1) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated sigma from linear model")

scap_beta_n <- ggplot(data = parameters_b, aes(x=n, y=beta)) + geom_point(shape=".") + ggtitle("Scatter plot of simulated n and beta from linear model") + theme(plot.title = element_text(size = 8))

scap_sigma_n <- ggplot(data = parameters_b, aes(x=n,y=sigma)) + geom_point(shape=".") + ggtitle("Scatter plot of simulated n and sigma from linear model") + theme(plot.title = element_text(size = 8))

grid.arrange(hist_n_l, hist_b_l, hist_s_l, hist_p_l, scap_beta_n, scap_sigma_n, ncol=2)
```

## Null plot simulation

This is the null scenario in our first experiment, eg. the two variables under tested are independent of each other. If the data arises from this situation, then the data plots will not show any systematic patterns theoretically. 

The model is designed the same as the linear model:
$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$
with elements of the model generated using the same specification as the linear model, except  

- $\beta_1 = 0$  

The coefficient of $X_i$ is always zero. So $X_i$ and $Y_i$ are uncorrelated of each other.

Figure \ref{fig:norela} are four example plots generated using the specifications above. Same as the linear model simulation, a total number of 240,000 data sets are simulated under this structure. The histograms of the simulated $n, \beta, \sigma$, the estimated p value and the scatter plots of $\beta$ against n, $\sigma$ against n in figure \ref{fig:simplot2} show good coverage over all the values.

```{r norela, echo=F, fig.height=6, fig.width=6, fig.cap="Four examples of data plots generated with two independent variables"}
norela<-function(i){
  n <- sample(c(100, 300), 1)
  beta <- 0
  sigma <- sample(1:12 , 1)
  x<-rnorm(n, 0, 1)
  y<-rnorm(n, beta*x, sigma)
  pic <- tibble(x, y) %>% 
    ggplot(aes(x = x, y=y)) + 
    geom_point(alpha = 0.4) +
    theme(aspect.ratio = 1)
}
set.seed(0518)
p1 <- norela(1)
p2 <- norela(2)
p3 <- norela(3)
p4 <- norela(4)
grid.arrange(p1,p2,p3,p4, nrow=2)
```

```{r simplot2, fig.height=6, fig.width=6, fig.cap="Overview of parameter values used in the null class simulation, for computer model training. Good coverage is obtained across the parameter space."}
parameters_0 <- read.csv("data/parameters_0.csv")
hist_n_0 <- ggplot(data = parameters_0, aes(n)) + geom_histogram(binwidth = 50) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated n from null model")

hist_p_0 <- ggplot(data = parameters_0, aes(ct.p.value)) + geom_histogram(binwidth = 0.1) +
  xlab("p-value") + 
  theme(plot.title = element_text(size = 8)) + 
  ggtitle("Histogram of estimated p value of t-test from null model")

hist_s_0 <- ggplot(data = parameters_0, aes(sigma)) + geom_histogram(binwidth = 1) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated sigma from null model")

scap_sigma_n0 <- ggplot(data = parameters_0, aes(x=n,y=sigma)) + geom_point(shape=".") + ggtitle("Scatter plot of simulated n and sigma from null model") + theme(plot.title = element_text(size = 8))

grid.arrange(hist_n_0, hist_s_0, hist_p_0, scap_sigma_n0, ncol=2)
```

## Computer model  

A convolutional neural network will be used to build the computer model because it proves powerful in all kinds of computer vision applications. And it has two interesting properties: "the patterns they learn are translation invariant", and "they can learn spatial hierarchies of patterns" [@DLR18]. Given the first one, once it learn how to recognize the linear pattern, they can detect it regardless of the direction of that patter. Thus it can handle different slopes. As to the second one, it allows the convnets to learn complicated visual concepts. 

All convolutional neural network related work will be done by Keras [@keras] package in R. The plots used for training and testing in this section is the scatter plot between the dependent variable Y and the independent variable X. The R package @ggplot2 is used to generate the plots. All plots are saved as png and will be resized to have width and height both equal to 150 pixels. This size is similar to the plot size used in the lineup for human evaluation. As for the labels given to each image, we use the true population as the samples' identification directly. It is true that there will be some undesired patterns formed out of randomness, especially when the sample size is small. Unlike what Simchoni did in his post, no conventional tests will be used to sort out the "significant" observations. Because the answer to the question that if the deep learning model can distinguish from patterns formed by chance and by nature is also interested. 

As mentioned above, 240,000 data sets are generated for each of the two groups in the first experiment. 100,000 of them are set apart for training. Another 40,000 of the data sets are set apart as validation set in order to monitor during training the accuracy of the model on data it has never seen before. And the leftover (100,000 data sets) become the unseen test set. We make the test set so large that we can compare the performance of the convnet with the conventional t-test properly.

"A convnet takes as input tensors of shape (image height, image width, image channels)."[@DLR18] The channels are normally equal to three for RGB. In our case, the input tensors are of shape $150 \times 150 \times 1$ because they are grayscale images. Therefore the convnet will be configured to process inputs of size (150, 150, 1). We’ll do this by passing the argument input_shape = c(28, 28, 1) to the first layer.

```{r modelstructure, echo=TRUE, fig.height=6, results='asis' ,warning=FALSE}
library(keras)
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), 
                activation = "relu",
                input_shape = c(150, 150, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), 
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), 
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), 
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
model
```

As shown in the table above, the "conv" and "pooling" working together slided the data from $150 \times 150 \times 1$ to $7 \times 7 \times 128$ (3D output). The figure \ref{conv} from @DLR18 decribes this transformation. 

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/conv.png}}
\caption{How convolution works (Figure is from 'Deep Learning with R' by Allaire, JJ)}
\label{conv}
\end{figure*}

Then we need to flatten these 3D tensor into 1D tensor so that they can be processed by the "sigmoid" function in the end. The "sigmoid" is in fact a special case of logistic function. $S(x)=\frac{1}{1+e^{-x}}$. This function is the same one used to predict $\hat{y_i}$ and to calculate the cost function.

From the model structrue we can see that a total number of 3,452,545 parameters need to be estimated, this is done by gradient descent. 10 epoches (1 epoch = 1 iteration over all samples) are done for training. The model specification given by each epoch is saved, the one gives the overall best performance is chosen to represent the computers. Because the plot of the training history (figure \ref{histlinear}) shows overfitting starts from the fourth epoch, the values of accuracy and loss from validation set are very close after the fourth epoch.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/linear_history_plot.png}}
\caption{Training and validation metrics of linear vs. null model in our first experiment}
\label{histlinear}
\end{figure*}

Hence, we select the fourth, sixth, eighth and the tenth model to have them tested on the unseen test set. And the results are shown in the table below. The "$1-\alpha$" is the accuracy of each computer model tested on the "null data" in test set only. $\alpha$ here is an analogy to the Type I error in the conventional hypothesis test. Similarly, the "power" is the accuracy of each computer model tested on the "linear data" in the test set only. The t-test performance is calculated under 5% significance level. 

<!-- table display is weird

```{r checkpoints, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
checkpoints <- c("4 epoch", "6 epoch", "8 epoch", "10 epoch","5% T-test")
powers <- c(0.8916, 0.8885, 0.8958, 0.9042, 0.9206)
alpha_1 <- c(0.9842, 0.9858, 0.9812, 0.9712, 0.9489) 
accuracy <- c(0.9379, 0.9372, 0.9385, 0.9377, 0.9348)
tbl <- tibble(checkpoints, powers, alpha_1, accuracy) 
kable(tbl, col.names = c("Tests", "Power", "1-Alpha", "Overall accuracy"), caption = "Performance of four checkpoints from the convnets as well as 5% significant t-test tested on the unseen test set, where the power means the accuracy on the linear ones, (1-alpha) is the accuracy on the no-relationship ones. All numbers have been rounded to four decimal places.")
```
-->
\begin{table}[ht]
\begin{center}
\begin{tabular}{|l|rrr|}\hline
Tests & Linear & Null & Overall \\\hline
4 epoch & 0.892 & 0.984 &  0.938 \\   
6 epoch & 0.889 & 0.986 & 0.937 \\
8 epoch & 0.896 & 0.981 & 0.939 \\
10 epoch & 0.904 & 0.971 & 0.938 \\
5\% $t$-test & 0.921 & 0.949 & 0.935\\\hline
\end{tabular}
\end{center}
\caption{Performance of four checkpoints from the {\em convnets} model, and the 5\% significant $t$-test, computed on the test set. Accuracy is reported for each class, and overall. There is a slight improvement as the number of epochs increases, with 10 epochs being reasonably close to the ideal $t$-test accuracy.}
\label{checkpoints}
\end{table}

Since the test set is large enough (200,000 in total) to provide reliable reference. The 8th model is chosen according to the overall accuracy on the test set.

For a fair competition, the Type I error ($\alpha$) should be the same for all test methods. However, we do not have direct control over the $\alpha$ of the computer model. 
Since the majority of the data plots in Turk's experiment have been generated with linear relationship (when the alternative hypothesis is true), this is a disadvantage for the computer comparing to the 5% significant t-test in terms of being tested on the Turk's data. Because of the difference in $\alpha$ ($\alpha \approx 0.02$ for the 8th computer model) the t-test has higher power than the computer model even though their overall accuracy are very close to each other.

Therefore, 2% significant t-test and 2% significant human conclusion is also included to give a complete picture of the comparison. 

## Comparing results  

First compare the experiment process for human and computer respectively by two diagrams figure \ref{dgpc} and figure \ref{dfhm}.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/diagpc.png}}
\caption{Diagram illustrating the training, diagnosis and choice of the computer model. Based on 240,000 simulated data sets used to create $150\times 150$ pixel images, divided into training, validation and test sets.}
\label{dgpc}
\end{figure*}

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/diaghm.png}}
\caption{Process of human experiment and results calculation}
\label{dghm}
\end{figure*}

The conclusion of human evaluation is obtained using the p-value calculation introduced in section 2 of @MM13, the null hypothesis is rejected if the calculated p-value is smaller than 0.05. 

```{r linearresults, echo=F, paged.print=FALSE, results='asis'}
tests <- c("Human 5%", "Human 2%", "T-test 5%", "Computer 2%", "T-test 2%")
rank <- c(1,1,2,3,4)
correct <- c(47, 47, 43, 39, 39)
accuracy <- c(0.6714,0.6714, 0.6143, 0.5571, 0.5571)
tibble(rank, tests, correct, accuracy) %>% kable(col.names = c("Rank","Tests", "No. of correct", "Accuracy"), caption = "Accuracy of testing the 70 data plots evaluated by human computer and the conventional t-test.")
```

The comparing result is interesting. Human achieves the highest accuracy, and the conclusion from human evaluation is robust to smaller p-values; 5% significant t-test is the second best, 2% significant t-test and the computer model perform similarly.

## Aside discussion related to the comparing results

As we can see from the two performance table above, t-test and convnets behave quite similarly on both the test data set and Turk's experiment data. 

<!-- graph -->

It is possible that the convnets is in fact doing the same thing as t-test in this case. 
To confirm this idea, a small simulation experiment was conducted to search for the "best" $\alpha$ for t-test. The data was simulated using the same specification as in the section "Linear relationship simulation" and "null plots simulation". The range of the $\alpha$ is from $(0.005,\ 0.010,\ 0.015,\ \dots,\ 0.100)$, with 200,000 iterations, the "best" $\alpha$ is found to be $\alpha=0.02$ which is very close to the one suggested by the fourth-epoch model $0.02435$.





