---
chapter: 2
knit: "bookdown::render_book"
---

# Experimental design {#ch:modesign}

Steps in the experiment are:

1. Simulate data from the different models
2. Fit a linear model to the data, extract standardized residuals and fitted values
3. Save residual plots as fixed-sized images
4. Train a deep learning classifier to recognise the departures from assumptions
5. Test the model's performance and compute the accuracy

## Data simulation {#sec:simulation}

These factors are being controlled in the data simulation

- Type of relationship: linear, nonlinear or heteroskedasticity
- Explanatory variables: $X \sim N(0,3)$ and intercept $\beta_0=0$
- Sample size: randomly generated between 20-1500
- Image size: fixed `150x150`

<!--
**ADD: explanations of choices** 
-->

Three groups of models are included in the two hypothesis tests, linear, nonlinear and heteroskedasticity. So the residual plots from these three models are the target to be designed. 

Although we want to perform this test under multivariate regression setting, we will use only one X variable to generate all different kinds of plots for training. Because the "statistics" we will use is the residual plot, one X variable is enough for us to generate different patterns in that plot for convnets to learn. And this also makes the design process much simpler. As for generation of X, we need to consider the distributions of X since it does have an impact on the shape of the scatters. For instance, if X are generated from a uniform distribution, then the plots will look like a square especially when the sample size is large; while more like a circle if X follows normal distribution. In this paper, we are going to fix the distribution of X variables as the same normal distribution $N(0,3)$ for all three groups of data. This way, we treat the X variables as non-random as what we do in practice, so we can compare different relationships of X and Y more easily.

We will set intercept to be zero for all three models, since it has no impact on the patterns of residual plots.

For each group 900 simulated data sets will be generated at first to facilitate deep learning model modification. (The data sets will be augmented later on for training.) 
The sample sizes will be 900 random numbers generated from 20 to 1500. These sample sizes will be kept the same for all the three groups. We choose 20 to 1500, because when the sample size is smaller than 20, say, 10, there is hardly any patterns to see (as shown in figure 2.1). And 1500 is large enough to give a good description about the true relationship within the data, and is light enough to be processed. Since we make the transparency of the points to be 0.01, more points being added to the plot will just make the plot looks darker. Note the upper bound of the sample size is subject to possible changes according to our future study. 

We set image size to be 150 $\times$ 150 pixels, because this size is able to show the plots clearly and also reasonably small so the images can be saved and processed easily.

<!--

Attention: plots layout.

```{r sample10, echo=FALSE, fig.cap="Three residual plots of sample size 10 generated from different models, hard to see any systematic patterns", message=FALSE, warning=FALSE}
classic<-function(n){
  beta<-runif(1,0.5,1.5)
  x<-runif(n, -3, 3)
  y<-rnorm(n, beta*x, 1)
  
  tibble(x, y)
  model<-lm(y ~ x)
  res<-residuals(model)
  yhat<-predict(model)
  res<-(res-mean(res))/sd(res)
  yhat<-(yhat-mean(yhat))/sd(yhat)
  return(tibble(res, yhat))
}

heter<-function(n){
  lambda <- runif(1,-1,1)
  beta<-runif(1,0.5,1.5)
  a <- abs(lambda)
  b <- rnorm(1)
  c <- rnorm(1)
  
  x<-runif(n, -3, 3)
  m <- mean(x)
  
  index <- sample(c(1:3),1)
  if (index==1){
    y <- rnorm(n, beta*x, exp(x*lambda)+rexp(n))
  } else if (index==2) {
    y <- rnorm(n, beta*x, a*x^2+b*x+c+abs(c-b^2/(2*a))+rexp(n))
  } else {
    y <- rnorm(n, beta*x, dnorm(x, mean = m)+rexp(n))
  }
  
  tibble(x, y)
  model<-lm(y ~ x)
  res<-residuals(model)
  yhat<-predict(model)
  res<-(res-mean(res))/sd(res)
  yhat<-(yhat-mean(yhat))/sd(yhat)
  return(tibble(res, yhat))
}

nonpoly<-function(n){
  p1<-rnorm(1,0,2)
  p2<-rnorm(1,0,2)
  p3<-rnorm(1,0,2)*rbinom(1,1,0.5)
  p4<-rnorm(1,0,2)*rbinom(1,1,0.3)
  p5<-rnorm(1,0,1)*rbinom(1,1,0.2)
  
  x<-runif(n,-3,3)
  y<-rnorm(n, p1*x+p2*x^2+p3*x^3+p4*x^4+p5*x^5, p1^2+2*p2^2+2*p3^2+2*p4^2+2*p5^2)
  
  tibble(x, y)
  model<-lm(y ~ x)
  res<-residuals(model)
  yhat<-predict(model)
  res<-(res-mean(res))/sd(res)
  yhat<-(yhat-mean(yhat))/sd(yhat)
  return(tibble(res, yhat))
}
set.seed(0404)

c10 <- classic(10) %>% 
  ggplot(aes(yhat, res))+geom_point()

h10 <- heter(10) %>% 
  ggplot(aes(yhat, res))+geom_point()

p10 <- nonpoly(10) %>% 
  ggplot(aes(yhat, res))+geom_point()

library(gridExtra)
grid.arrange(c10, h10, p10, ncol=1)
```
-->

### Models for relationship

#### Classic linear model 

This is the null scenario, eg. all Gauss-Markov assumptions are true in the original linear model of X and Y. If the data arises from this model, then the residual plots will not show departures from the assumptions. However, there may still be some undesired patterns formed out of randomness, especially when the sample size is small. Unlike what Simchoni did in his post, we will not use any conventional tests to sort out the "significant" observations. Imagine when you have millions of observations from a null distribution, the plot will just look like a black block in the middle and show no pattern at all, where the visual inference can be 100% sure to accept the null, while the convetional tests will still rejects 5% of these observations regardless based on its presetting. Hence, we will simply use the true parent model as the residuals' identification directly and ignore those noises.

$$Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i, ~~i=1, \dots , n$$

with elements of the model generated by the following processes

- $X\sim N[0,3]$
- $\beta_0 = 0$
- $\beta_1\sim U[0.5, 1]$
- $\varepsilon\sim N(0, 1)$
- $n\sim U[20, 1500]$

**ADD: explanation of choices, eg error variance=1**

After standardization, all residuals will follow standard normal distribution. Therefore, variance of the error term will not make any differences to the residual plots, neither does the correlation coefficient between X and Y. So we will set variance of error term to be one, then randomly select a correlation from 0.5 to 1.  Figure 2.2 are four examples of residual plots generated under classic linear model.


```{r classic_plot, message=FALSE, cache=TRUE}
library(gridExtra)
classic<-function(n){
  
  beta<-runif(1,0.5,1)
  
  x<-rnorm(n, 0, 3)
  y<-rnorm(n, beta*x, 1)
  
  tibble(x, y)
  model<-lm(y ~ x)
  res<-residuals(model)
  yhat<-predict(model)
  res<-(res-mean(res))/sd(res)
  yhat<-(yhat-mean(yhat))/sd(yhat)
  return(tibble(res, yhat))
}
set.seed(0405)
N <- sample(20:1500, 4, replace=TRUE)
  cls <- tibble(N) %>% 
    mutate(res=map(N, classic))
    cls <- cls$res
    
p1 <- ggplot(cls[[1]], aes(x=yhat, y=res), alpha=0.01) + 
  geom_point() + 
  xlab("Fitted values") + ylab("Residuals")
p2 <- ggplot(cls[[2]], aes(x=yhat, y=res), alpha=0.01) + 
  geom_point() + 
  xlab("Fitted values") + ylab("Residuals")
p3 <- ggplot(cls[[3]], aes(x=yhat, y=res), alpha=0.01) + 
  geom_point() + 
  xlab("Fitted values") + ylab("Residuals")
p4 <- ggplot(cls[[4]], aes(x=yhat, y=res), alpha=0.01) + 
  geom_point() + 
  xlab("Fitted values") + ylab("Residuals")
  
grid.arrange(p1,p2,p3,p4,nrow=2, heights=c(1,1)) 

```


#### Linear Model with Heteroskedasticity {#sec:heter}

**CHANGE THIS SECTION TO BE EXPLICIT LIKE THE CLASSIC MODEL**

This model is designed to present one of the alternative hypothesis, eg. when the constant variance assumption is violated holding all other conditions unchanged.

Hence, the only difference needs to be made for this model is to add a non-zero relationship between the variance of Y and X. It is impossible to list out all types of heteroskedasticity by a simple function form. Here I am trying to present four kinds of relationship between variance of Y and X using a second order polynomial. With randomly generated parameters, the variance of Y could be monotonically increasing/decreasing with X, or has a turning point in the middle.

$$Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i, ~~i=1, \dots , n$$

with elements of the model generated by the following processes

- $X\sim N[0,\ 3]$
- $\beta_0 = 0$
- $\beta_1\sim U[0.5,\ 1]$
- $\varepsilon_i\sim N(0,\ ax_i^2+bx_i+c+v_i)$
- $v_i \sim N(0,\ 0.25)$
- $n\sim U[20,\ 1500]$

Choices of the distribution to generate a, b, c and v are basically an empirical work. With freedom of choosing a and b, all four types of relationships can be realized. Primarily, we want the residual plots to show variations among different data sets; on the other hand, we need to take a bit control on the choices of these parameters, prevent the final variance from being too large so the linear relationship between X and Y will not disappear. Figure 2.3 are four examples of residual plots generated under this model.


```{r heter_plot, message=FALSE, cache=TRUE}
heter<-function(n){
  
  x<-rnorm(n, 0, 3)
  beta<-runif(1,0.5,1)
  
  a <- runif(1,0,2)*rbinom(1,1,0.5)
  b <- runif(1,-4,4)
  c <- rnorm(1,0,2)
  
  variance <- 0.25*(a*x^2+b*x+c)+rnorm(n,0,0.25)
  
  index <- sample(0:1,1)
  if (index==1) {
    variance <- -variance
  }
  
  min <- min(variance)
  
  if (min<0){
    variance <- variance-min
  }
  
  max <- max(variance)
  while (max>5) {
    variance <- 0.8* variance
    max <- max(variance)
  }
  
  y<-rnorm(n, beta*x, variance)
  
  tibble(x, y)
  model<-lm(y ~ x)
  res<-residuals(model)
  yhat<-predict(model)
  res<-(res-mean(res))/sd(res)
  yhat<-(yhat-mean(yhat))/sd(yhat)
  return(tibble(res, yhat))
}
set.seed(0003)
N <- sample(20:1500, 4, replace=TRUE)
  het <- tibble(N) %>% 
    mutate(res=map(N, heter))
    het <- het$res

    
p1 <-   ggplot(het[[1]], aes(x=yhat, y=res), alpha=0.01) +geom_point()
p2 <-   ggplot(het[[2]], aes(x=yhat, y=res), alpha=0.01) +geom_point()
p3 <-   ggplot(het[[3]], aes(x=yhat, y=res), alpha=0.01) + geom_point()
p4 <-   ggplot(het[[4]], aes(x=yhat, y=res), alpha=0.01) + geom_point()
  
grid.arrange(p1,p2,p3,p4,nrow=2, heights=c(1,1)) 

```

#### Non-linear model {#sec:poly}

This data set will be designed to be representative of non-linear relationship between X and Y, keep all other factors being the same with the classic linear model.

Obviously we cannot write down all different non-linear relationships between X and Y. Luckily, we do not need to actually do that. Because our purpose is for the model to tell if there is any kind of non-linear patterns, but not to specify which type it is. As long as the relationship between residual plots is curved, we do not worry about if it is from a logarithm function or squared root. Therefore all we want is for the model to "see" varied patterns, we just need to find a way to show different curvature. Hence, at this stage, we only include a polynomial of X to the order of five with each parameter being randomly generated. I chose five because normally the order of the non-linear relationship of data from Economics world do not exceed four. What's more, we find this design is good enough to vary the pattern of our plots. Also, since order four and five are not as common as lower orders, we attach a bernoulli term with 0.5 success rate to them respectively. 

So the Y will be a normal distribution with mean
$\beta_1x+\beta_2x^2+\beta_3x^3+\beta_4x^4+\beta_5x^5$, and variance $\beta_1^2+\beta_2^2+\beta_3^2+2\beta_4^2+2\beta_5^2$. We adjust the variance to be increase for higher orders so the data could has more variation. Below are four examples generated from this model.

```{r poly_plot, message=FALSE, cache=TRUE}
nonpoly<-function(n){
  
  p1<-rnorm(1,0,2)
  p2<-rnorm(1,0,2)
  p3<-rnorm(1,0,2)*rbinom(1,1,0.5)
  p4<-rnorm(1,0,2)*rbinom(1,1,0.3)
  p5<-rnorm(1,0,1)*rbinom(1,1,0.2)
  
  x<-runif(n,-3,3)
  y<-rnorm(n, p1*x+p2*x^2+p3*x^3+p4*x^4+p5*x^5, p1^2+2*p2^2+2*p3^2+2*p4^2+2*p5^2)
  
  tibble(x, y)
  model<-lm(y ~ x)
  res<-residuals(model)
  yhat<-predict(model)
  res<-(res-mean(res))/sd(res)
  yhat<-(yhat-mean(yhat))/sd(yhat)
  return(tibble(res, yhat))
}

set.seed(0002)
N <- sample(20:800, 4, replace=TRUE)
  pl <- tibble(N) %>% 
    mutate(res=map(N, nonpoly))
    pl <- pl$res
    
p1 <-   ggplot(pl[[1]], aes(x=yhat, y=res, alpha=0.01)) +geom_point()
p2 <-   ggplot(pl[[2]], aes(x=yhat, y=res, alpha=0.01)) +geom_point()
p3 <-   ggplot(pl[[3]], aes(x=yhat, y=res, alpha=0.01)) +geom_point()
p4 <-   ggplot(pl[[4]], aes(x=yhat, y=res, alpha=0.01)) +geom_point()
  
grid.arrange(p1,p2,p3,p4,nrow=2, heights=c(0.6,0.6)) 

```


## Extract residuals and fitted values



## Scale data into fixed image size

## Train the deep learning classifier

## Assess the accuracy



























