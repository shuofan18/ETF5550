---
chapter: 2
knit: "bookdown::render_book"
---

# Comparing computer performance against the database of human evaluation

A database of human evaluations of scatterplots is available from prior studies. This is used to compare the performance of the computer model. The computer model is trained on a broader parameter simulation framework and tested on the same data as the human evaluations. 

## Amazon Mechanical Turk study

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/plot_turk2_300_350_12_3.png}}
\caption{One of 70 lineups used in experiment 2 Majumder et al (2012). Of the 65 people who examined the lineup,  63 selected the data plot, which is in position 20.}
\label{expt2}
\end{figure*}

A large database of results from human subjects was collected examine the performance of the lineup protocol relative to classical tests. The work is published in @MM13. This database forms the basis of the test set used to examine the computer model performance. 

In @MM13, "three experiments were conducted to evaluate the effectiveness of the lineup protocol relative to the equivalent test statistic used in the regression setting." In each experiment, they simulated data from a controlled setting and then generated associated lineup for the human to evaluate. The human subjects were hired from Amazon Mechanical Turk where is a marketplace for work that requires human intelligence.

The controlled model in their first experiment is 
$$Y_i=\beta_0+\beta_1 X_{i1}+\beta_2 X_{i2}+\epsilon_i$$ where $\beta_0=5, \beta_1=15, X_1 \sim Poisson(\lambda=30), \epsilon_i\sim N(0,\sigma^2), i=1,2,...,n$, $\beta_2$ used in generating real data is specified in table 2.1. While in the null model $\beta_2=0$, and the null data was generated by simulating from $N(0,\hat{\sigma}^2)$. This experiment was aimed to test the ability of human on detecting the effect of $X_2$.

Their second experiment is very similar to the first one, but there is only one continuous variable $X_1$ on the right-hand side. It examined the performance of humans in recognising linear association between two variables, in direct comparison to conducting a $t$-test of $H_o: \beta_k=0$ vs $H_a: \beta_k\neq 0$ assessing the importance of including variable $k$ in the linear model. The actual data model is
$$Y_i=\beta_0+\beta_1 X_{i1}+\epsilon_i$$ where $\beta_0=6, X_1\sim N(0,1)$, and the null data was generated from $N(0, \hat{\sigma}^2)$.

The third experiment in their paper contains contaminated data where the actual data were in fact generated from two different specifications. 
$$Y_i=
  \begin{cases}
    \alpha+\beta X_i+\epsilon_i       & \quad X_i\sim N(0,1)\ \ i=1,...,n\\
    \lambda+\eta_i  & \quad X_i\sim N(\mu,1/3)\ \ i=1,...,n_c
  \end{cases}
$$ where $\epsilon_i \sim N(0,\sigma), \eta_i \sim N(0,\sigma/3), \ \mu=-1.75, \ \beta\in(0.1, 0.4, 0.75, 1.25, 1.5, 2.25)$. And $n=100, n_c=15, alpha=0, \lambda=10, \sigma=3.5$. The null plots were generated from $N(0,\hat{\sigma}^2)$.

Other parameters in the "actual" data sets of Turk experiment one and Turk experiment two are shown in the table 2.1. In this study, we will mainly focus on the their second experiment and use its database to form our first comparison experiment. This experiment 2 utilized 70 lineups of size 20 plot, with varying degrees of departure from the $H_o: \beta_k=0$. There were 351 evaluations by human subjects. These results will be used for comparison with the deep learning model. An example lineup question in Turk experiment 2 is shown in Figure \ref{expt2}. For this lineup, 63 of the 65 people who examined it selected the data plot (position 20) from the null plots. There is clear evidence that the data displayed in plot 20 is not from $H_o: \beta_k=0$. 

```{r turktable,ref.label='turktable', echo=F, results='asis'}
c1 <- c(100,100,300,300)
c2 <- c(5,12,5,12)
c3 <- c("0,1,3,5,8", "1,3,8,10,16", "0,1,2,3,5", "1,3,5,7,10")
c4 <- c("0.25, 0.75, 1.25, 1.75, 2.75", "0.5, 1.5, 3.5, 4.5, 6", "0.1, 0.4, 0.7, 1, 1.5","0, 0.8, 1.75, 2.3, 3.5")
tibble(c1,c2,c3,c4) %>% kable(col.names = c("Sample size (n)", "Error SD(sigma)", "Experiment 1 beta2", "Experiment 2 beta1"), align = "c", caption = "Combination of parameter values used for simulation in Turk's study.")
```

## Linear relationship simulation

The design for our model under the alternative hypothesis in the first experiment is similar to what @SIM18 did in his blog. But the parameters are tailored to compare the computer performance with the Turk study results.

The model under the alternative is designed as:
$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$
And all the parameters in our model were designed to cover the range used in the second experiment in Turk study [@MM13]. Therefore, the relevant parameters in our model are generated using the following specification.

- $X \sim N[0,\ 1]$  
Distributions of X has an impact on the shape of the scatters. For instance, if X is generated from a uniform distribution, then the plots will look like a square when the sample size is large; while look like a circle if X follows a normal distribution. 

- $\beta_0 = 0$  
Intercept is arbitrarily set to be zero because it has no impact on the patterns in the data plots.

- $\beta_1\sim U[-10, -0.1] \bigcup [0.1, 10]$  
$\beta_1$ is designed to be uniformly generated from -10 to 10 (excluding -0.1 to 0.1).

- $\varepsilon\sim N(0, \sigma^2) \ where\ \sigma \sim U[1,12]$  
$\varepsilon$ is designed to be uniformly distributed from 1 to 12.

- $n=U[50,500]$  
The sample sizes of each data set vary from 50 to 500 observations.

Figure \ref{fig:linear} shows four example plots generated using the specifications above. To facilitate the computer vision, all texts, ticks and titles of X and Y axes are removed, so does the background grid. Under this controlled structure, a total number of 240,000 datasets are simulated. Figure \ref{fig:simplot} contains a histogram of the simulated n, a histogram of the simulated $\beta$, a histogram of the simulated $\sigma$, a histogram of the estimated sample p-value, a scatter plot of $\beta$ against n and a scatter plot of $\sigma$ against n. These plots show good coverage over the alternative parameter space.

```{r linear, fig.height=6, fig.width=6, echo=FALSE, fig.cap="Four examples of data plots generated from the classic linear model."}
library(gridExtra)
linear<-function(n){
   beta <- sample(c((-10:-0.1),(0.1:10)), 1)
  
  sigma <- sample(1:12 , 1)
  x<-rnorm(n, 0, 1)
  y<-rnorm(n, beta*x, sigma)
  
  tibble(x, y) %>% 
  ggplot(aes(x = x, y = y)) + 
    geom_point(alpha = 0.4) +
    theme(aspect.ratio = 1)
}
set.seed(0420)
n1 <- sample(c(100, 300), 1)
n2 <- sample(c(100, 300), 1)
n3 <- sample(c(100, 300), 1)
n4 <- sample(c(100, 300), 1)

p1 <- linear(n1)
p2 <- linear(n2)
p3 <- linear(n3)
p4 <- linear(n4)

grid.arrange(p1,p2,p3,p4,nrow=2, heights=c(1,1)) 

```


```{r simplot, fig.height=8, fig.width=6, fig.cap="Overview of parameter values used in the linear class simulation, for computer model training. Good coverage is obtained across the parameter space."}
parameters_b <- read.csv("data/parameters_b.csv")
hist_n_l <- ggplot(data = parameters_b, aes(n)) + geom_histogram(binwidth = 50, center = 25) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated n")

hist_b_l <- ggplot(data = parameters_b, aes(beta)) + geom_histogram(binwidth = 1, center=-10.5) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated beta from linear model")

hist_p_l <- ggplot(data = parameters_b, aes(ct.p.value)) + geom_histogram(binwidth = 0.1) +
  xlab("p-value") + 
  theme(plot.title = element_text(size = 8)) + 
  ggtitle("Histogram of estimated p value of t-test from linear model")

hist_s_l <- ggplot(data = parameters_b, aes(sigma)) + geom_histogram(binwidth = 1, center=-0.5) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated sigma from linear model")

scap_beta_n <- ggplot(data = parameters_b, aes(x=n, y=beta)) + geom_point(shape=".") + ggtitle("Scatter plot of simulated n and beta from linear model") + theme(plot.title = element_text(size = 8))

scap_sigma_n <- ggplot(data = parameters_b, aes(x=n,y=sigma)) + geom_point(shape=".") + ggtitle("Scatter plot of simulated n and sigma from linear model") + theme(plot.title = element_text(size = 8))

grid.arrange(hist_n_l, hist_b_l, hist_s_l, hist_p_l, scap_beta_n, scap_sigma_n, ncol=2)
```

## Null plot simulation

This is the null scenario in our first experiment, eg. the two variables under tested are independent of each other. If the data arise from this situation, then the data plots will not show any systematic patterns theoretically. It is true that there must be some undesired patterns formed out of randomness, especially when the sample size is small. Unlike what Simchoni did in his post, no conventional tests will be used to sort out the "significant/insignificant" samples. Because the answer to the question that if the deep learning model can distinguish from patterns formed by chance and by nature is also interesting. The model is designed the same as the linear model:
$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$
with elements of the model generated using the same specification as the linear model, except  
- $\beta_1 = 0$  

e.g. the coefficient of $X_i$ is always zero. So $X$ and $Y$ are uncorrelated of each other.

Figure \ref{fig:norela} are four example plots generated using the specifications above. Same as the linear model simulation, a total number of 240,000 datasets are simulated under this structure. Figure \ref{fig:simp} contains a histogram of the simulated n, a histogram of the simulated $\sigma$, a histogram of the estimated sample p-value and a scatter plot of $\sigma$ against n. These plots show good coverage over the null parameter space. All simulated data and associated parameters including estimated sample p-values of t-test are saved and are used later on for calculating the performance of conventional t-test. 

```{r norela, echo=F, fig.height=6, fig.width=6, fig.cap="Four examples of data plots generated with two independent variables"}
norela<-function(i){
  n <- sample(c(100, 300), 1)
  beta <- 0
  sigma <- sample(1:12 , 1)
  x<-rnorm(n, 0, 1)
  y<-rnorm(n, beta*x, sigma)
  pic <- tibble(x, y) %>% 
    ggplot(aes(x = x, y=y)) + 
    geom_point(alpha = 0.4) +
    theme(aspect.ratio = 1)
}
set.seed(0518)
p1 <- norela(1)
p2 <- norela(2)
p3 <- norela(3)
p4 <- norela(4)
grid.arrange(p1,p2,p3,p4, nrow=2)
```

```{r simp, fig.height=6, fig.width=6, fig.cap="Overview of parameter values used in the null class simulation, for computer model training. Good coverage is obtained across the parameter space."}
parameters_0 <- read.csv("data/parameters_0.csv")
hist_n_0 <- ggplot(data = parameters_0, aes(n)) + geom_histogram(binwidth = 50, center = 25) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated n from null model")

hist_p_0 <- ggplot(data = parameters_0, aes(ct.p.value)) + geom_histogram(binwidth = 0.1, center=0.05) +
  xlab("p-value") + 
  theme(plot.title = element_text(size = 8)) + 
  ggtitle("Histogram of estimated p value of t-test from null model")

hist_s_0 <- ggplot(data = parameters_0, aes(sigma)) + geom_histogram(binwidth = 1, center=-0.5) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated sigma from null model")

scap_sigma_n0 <- ggplot(data = parameters_0, aes(x=n,y=sigma)) + geom_point(shape=".") + ggtitle("Scatter plot of simulated n and sigma from null model") + theme(plot.title = element_text(size = 8))

grid.arrange(hist_n_0, hist_s_0, hist_p_0, scap_sigma_n0, ncol=2)
```

## Computer model

All convolutional neural network related work is done by the Keras [@keras] package in R [@R], which interfaces to the python software. The plots used for training and testing in this section is the scatterplot between the dependent variable Y and the independent variable X. It can also be considered as the residual plot of such data fitting to a constant model. The R package ggplot2 [@ggplot2] is used to generate the plots. All plots are resized to $150\times 150$ pixel and saved as png. This size is similar to the plot size used in the lineup for human evaluation. As for the labels given to each image, we use the true population as the samples' identification directly.

As mentioned above, 240,000 data sets are generated for each of the two groups in the first experiment. 100,000 of them are set apart for training. Another 40,000 of the data sets are set apart as the validation set in order to monitor during training the accuracy of the model on data it has never seen before. And the leftover (100,000 data sets) become the unseen test set. We make the test set so large that we can compare the performance of the convnet with the conventional t-test properly.

"A convnets takes as input tensors of shape (image height, image width, image channels)."[@DLR18] The channels are normally equal to three for RGB. In our case, the input tensors are of shape $150 \times 150 \times 1$. The channel is equal to one because the input data is grayscale images. Therefore the convnet will be configured to process inputs of size (150, 150, 1). We’ll do this by passing the argument input_shape = c(150, 150, 1) to the first layer. The R codes below are used to build the convnets in R. From figure \ref{modelstruc}, the output shape changes after every layer of "conv" and "pooling" operations. The original $150 \times 150 \times 1$ image is finally sliced into a $7 \times 7 \times 128$ object (3D tensor). The figure \ref{diagconv} describes how "convolution" and "max pooling" operation works. By "convolution" the image matrix is multiplied by a filter, different filter gives different output matrix which extracting different features from the image. Max pooling select the max number within a certain area. Then we need to flatten these 3D tensor into 1D tensor so that they can be processed by the "sigmoid" function in the end. The "sigmoid" is, in fact, a special case of logistic function. $S(\textbf{x})=\frac{1}{1+e^{-\textbf{x}}}$. From this model structure, we can also see that a total number of 3,452,545 parameters need to be estimated, this is done by gradient descent. 10 epochs (1 epoch = 1 iteration over all samples) are done for training in our first experiment. The model specifications given by each epoch are saved, the one gives the overall highest accuracy is chosen to represent the computer. 

```{r modelstructure, echo=TRUE, fig.height=6, results='none', warning=FALSE, fig.cap="R codes used to build convolutional neural network."}
library(keras)
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), 
                activation = "relu",
                input_shape = c(150, 150, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), 
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), 
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), 
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/modelstruc.png}}
\caption{The deep learning model structure used for both of the experiments.}
\label{modelstruc}
\end{figure*}

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/diagconv.png}}
\caption{Illustration of convolution and pooling steps on an image. The convolution step applies a fixed number of filters to sliding windows of $3\times 3$ cells. Pooling applies a statistic to distinct $2\times 2$ tiling of the image. In our model, the statistic used is the maximum of the four values. These transformations are the pre-processing steps done on every image in the training sample, to fit the model, and also to the validation and test images prior to prediction. }
\label{diagconv}
\end{figure*}

The plot of the training history (figure \ref{histlinear}) shows high accuracy achieved in both train and validation set (93%-94%); slight overfitting starts from the fourth epoch; the variation of the values of accuracy and loss in validation set are very small after the fourth epoch. Hence, both our convnets and dataset are large enough and the training of our first experiment can be concluded. Then we select the fourth, sixth, eighth and the tenth model to have them tested on the unseen test set. And the results are shown in the table \ref{checkpoints}. In this table, the "$1-\alpha$" means the accuracy of each computer model tested on the "null data" in the test set only. $\alpha$ here is an analogy to the Type I error in the conventional hypothesis test. Similarly, the "power" is the accuracy of each computer model tested on the "linear data" in the test set only. The t-test performance in this table is calculated at 5% significance level. 

The 8th model is chosen according to the overall accuracy on the test set. We should note that since the majority of the data plots in Turk's experiment have been generated with linear relationship (when the alternative hypothesis is true), it is a disadvantage for the computer comparing in terms of being tested on the Turk's data. Because of the difference in $\alpha$ ($\alpha \approx 0.02$ for the 8th computer model) the 5% significant t-test and 5% human evaluations may have higher power than the computer model.

\begin{table}[ht]
\begin{center}
\begin{tabular}{|l|rrr|}\hline
Tests & Linear & Null & Overall \\\hline
4 epoch & 0.892 & 0.984 &  0.938 \\   
6 epoch & 0.889 & 0.986 & 0.937 \\
8 epoch & 0.896 & 0.981 & 0.939 \\
10 epoch & 0.904 & 0.971 & 0.938 \\
5\% $t$-test & 0.921 & 0.949 & 0.935\\\hline
\end{tabular}
\end{center}
\caption{Performance of four checkpoints from the {\em convnets} model, and the 5\% significant $t$-test, computed on the test set. Accuracy is reported for each class, and overall. There is a slight improvement as the number of epochs increases, with 10 epochs being reasonably close to the ideal $t$-test accuracy.}
\label{checkpoints}
\end{table}

## Comparing results

The performance of the computer model for the Turk study data is tested in three steps:

- Re-generate the 70 "real plots" using the same data in Turk study (without null plots);

- Create a separate test directory for the 70 "real plots" exclusively;

- The computer model's predicted accuracy over the 70 "real plots" is recorded as the model's performance.

The conclusion of human evaluation is obtained differently from the computers. Because human evaluated "lineup", not only the "real plots". The performance is tested in five steps:

- Count the total number of evaluations made by human for one lineup (N) and the number of correct answers for that lineup (k);

- Obtain N and k for all 70 lineups;

- Calculate p-value associated with each real plot using the formula introduced in section 2 of @MM13;

- Draw the conclusion: reject the null when the calculated p-value is smaller than $\alpha$.

- The accuracy of the conclusions the 70 real plots is presenting for the human performance.

For a fair competition, the Type I error ($\alpha$) should be held the same for all test methods. However, we do not have direct control over the $\alpha$ of the computer model. Because the $\alpha$ estimated from the computer model is close to 2%. Therefore, 2% significant t-test and 2% significant human conclusion are also included to give a complete picture of the comparison. The comparing result (table 2.3) is interesting. Human achieves the highest accuracy, and the conclusion from the human evaluation is robust to smaller p-values; 5% significant t-test is the second best, 2% significant t-test and the computer model give the same results. 

```{r linearresults, echo=F, paged.print=FALSE, results='asis'}
tests <- c("Human 5%", "Human 2%", "T-test 5%", "Computer 2%", "T-test 2%")
rank <- c(1,1,2,3,4)
correct <- c(47, 47, 43, 39, 39)
accuracy <- c(0.6714,0.6714, 0.6143, 0.5571, 0.5571)
tibble(rank, tests, correct, accuracy) %>% kable(col.names = c("Rank","Tests", "No. of correct", "Accuracy"), caption = "Accuracy of testing the 70 data plots evaluated by human computer and the conventional t-test.")
```

## Aside discussion

Under the condition specified in this chapter, the conventional t-test is supposed to be the uniformly most powerful (UMP) test in terms of detecting the linear relationship according to the Neyman–Pearson lemma. Although human achieved the best performance in the Turk experiment dataset, it does not mean computer does badly since the Turk experiment dataset only contains 70 plots. As we can see from table \ref{checkpoints}, t-test and convnets behave quite similarly on both the test data and Turk's experiment data. Given our test set is large enough (200,000 images totally), it is reasonable to assume that the convnets is, in fact, implementing the t-test or trying to approach the t-test. In other words, the best strategy the convnets learned, in this case, may turns out to be t-test. 

To check this idea, we calculated the accuracy of t-test again, with different $\alpha$ (from 0.005 to 0.1 with 0.005 increments) on all 200,000 test sets. The estimated power and overall accuracy were recorded. When $\alpha=0.015$, the overall accuracy reaches its maximum. This value approximately coincide with the $\alpha$ chosen by the convnets. And since the $\alpha$ of convnets is from 0.0142 to 0.0347 on the test set, we truncated the t-test data to create figure \ref{fig:ttdl}. The upper dots represent overall accuracy achieved by convnets (red) and t-test (green), while the lower dots stand for the estimated power in the test set. The smooth line overlaid aids the eye in seeing patterns. From this graph, we can see the convnets and t-test perform very similarly, while t-test has overall better performance.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/linear_history_plot.png}}
\caption{Training and validation metrics of linear vs. null model in our first experiment. The top plot is the accuracy achieved in train and validaton sets, while the bottom plot is the loss. Red presents model performance in train set while green presents validation.}
\label{histlinear}
\end{figure*}


```{r ttdl, echo=FALSE, fig.height=3, fig.width=6, fig.align='center', fig.cap="Comparison between computer model and t-test for alpha in (0.01, 0.04), they perform very similarly, but t-test has overall better performance."}

tt <- read.csv("~/documents/github/etf5550/linear&norela/ttest_test_set.csv")
tt <- select(tt, alpha, power, accuracy)
tt <- filter(tt, alpha<0.04)
tt <- filter(tt, alpha>0.01)
colnames(tt) <- c("alpha", "power", "accuracy")
tt <- tt %>% mutate(model="T-test")
dl <- read.csv("~/documents/github/etf5550/linear&norela/test_acc_10epoches.csv")
dl <- dl %>% mutate(alpha=1-alphas)
dl <- select(dl, alpha, powers, accuracy) 
colnames(dl) <- c("alpha", "power", "accuracy")
dl <- dl %>% mutate(model="Deep learning model")

tt_dl <- rbind(tt,dl)

library(reshape2)
tt_dl <- tt_dl %>% melt(id=c("model","alpha"))

ggplot(tt_dl, aes(x=alpha, y=value, group=variable, color=model))+
  geom_point(size=3)+
  geom_smooth(method = lm, formula = y ~ splines::bs(x, 3), se = FALSE)
```







