---
chapter: 2
knit: "bookdown::render_book"
---

# Comparing computer performance against database of human evaluation

## Turk study explanation

A large database of results from a human subjects test conducted to validate the lineup protocol relative to a classical tests is going to be used for this part of the work. This data was collected as part of the work presented in @MM13. Experiment 2 examined the performance of humans in recognising linear association between two variables, in direct comparison to conducting a $t$-test of $H_o: \beta_k=0$ vs $H_a: \beta_k\neq 0$ assessing the importance of including variable $k$ in the linear model. An example lineup is shown in Figure \ref{expt2}. For this lineup, 63 of the 65 people who examined it selected the data plot (position 20) from the null plots. There is clear evidence that the data displayed in plot 20 is not from $H_o: \beta_k=0$. 

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/plot_turk2_300_350_12_3.png}}
\caption{One of 70 lineups used in experiment 2 Majumder et al (2012). Of the 65 people who examined the lineup,  63 selected the data plot, which is in position 20.}
\label{expt2}
\end{figure*}

This experiment utilised 70 lineups of size 20 plot, with varying degrees of departure from the $H_o: \beta_k=0$. There were 351 evaluations by human subjects. These results will be used for comparison with the deep learning model. 

The trained deep learning model will be applied to the data from this experiment. The model will be asked to classify the 70 data plots. We will calculate how frequently the data plot is selected as not a null plot, and compare this to the frequencies obtained by human evaluation. 

## Linear relationship simulation

The design for this model is similar to what @SIM18 did in his blog but is tailored to perform the comparison with human performance later on.

The model is designed as:
$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$
with elements of the model generated by the following processes

- $X \sim N[0,\ 1]$  
Distributions of X has impact on the shape of the scatters. For instance, if X is generated from a uniform distribution, then the plots will look like a square especially when the sample size is large; while more like a circle if X follows normal distribution. In this experiment, we will set it to be normal to match with the design in the second experiment of @MM13. 

- $\beta_0 = 0$  
Intercept is set to be zero, because it will not change the data plots since all data will be standardized before being plotted. Any constant intercept has same effect on the data plots.

- $\beta_1\sim U[-10, -0.1] \bigcup [0.1, 10]$  
$\beta_1$ is designed to be uniformly generated from -10 to 10 (excluding -0.1 to 0.1) to cover the range used in the second experiment in @MM13.

- $\varepsilon\sim N(0, \sigma^2) \ where\ \sigma \sim U(1,12)$  
$\varepsilon$ is designed to be uniformly generated from 1 to 12 in order to cover the range used in the second experiment in @MM13.

- $n=U(50,500)$  
Number of observations is a random integer from 50 to 500 to cover all the sample sizes in the second experiment in @MM13.

```{r linear, echo=FALSE, fig.cap="Four examples of data plots generated from the classic linear model"}
library(gridExtra)
linear<-function(n){
   beta <- sample(c((-10:-0.1),(0.1:10)), 1)
  
  sigma <- sample(1:12 , 1)
  x<-rnorm(n, 0, 1)
  y<-rnorm(n, beta*x, sigma)
  
  tibble(x, y) %>% 
  ggplot(aes(x = x, y = y)) + 
    geom_point(alpha = 0.4) +
    theme(aspect.ratio = 1)
}
set.seed(0420)
n1 <- sample(c(100, 300), 1)
n2 <- sample(c(100, 300), 1)
n3 <- sample(c(100, 300), 1)
n4 <- sample(c(100, 300), 1)

p1 <- linear(n1)
p2 <- linear(n2)
p3 <- linear(n3)
p4 <- linear(n4)

grid.arrange(p1,p2,p3,p4,nrow=2, heights=c(1,1)) 
```

## Null plots simulation

This is the null scenario in the first experiment, eg. the two variables under tested are independent of each other. If the data arises from this situation, then the data plots will not show any systematic patterns theoretically. 

The model is designed as:
$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$
with elements of the model generated by the following processes

- $X \sim N[0,\ 1]$  
- $\beta_0 = 0$  
- $\beta_1 = 0$  
- $\varepsilon\sim N(0, \sigma^2) \ where\ \sigma \sim U(1,12)$  
- $n=100, 300$  

The specification of this model is the same with the linear relationship, the only difference is the correlation coefficient equals to zero in this case.

```{r norela, echo=F, fig.cap="Four examples of data plots generated with two independent variables"}
norela<-function(i){
  n <- sample(c(100, 300), 1)
  beta <- 0
  sigma <- sample(1:12 , 1)
  x<-rnorm(n, 0, 1)
  y<-rnorm(n, beta*x, sigma)
  pic <- tibble(x, y) %>% 
    ggplot(aes(x = x, y=y)) + 
    geom_point(alpha = 0.4) +
    theme(aspect.ratio = 1)
}
set.seed(0518)
p1 <- norela(1)
p2 <- norela(2)
p3 <- norela(3)
p4 <- norela(4)
grid.arrange(p1,p2,p3,p4, nrow=2)

```

## Computer model

As introduced in Chapter 1, the convolutional neural networks will be used to build the computer model. The plots used for training and testing in this section is the scatter plot between the dependent variable Y and the independent variable X. "ggplot2" package in R is used to generate the plots and to save all plots to our local drive. All plots will be resized to be squared images with same width and height. This helps us to see better the patterns in the scatter plot and also is easier for the deep learning model to process. As for the labels given the each image, we will use the true population as the samples' identification directly. It is true that there will be some undesired patterns formed out of randomness, especially when the sample size is small. Unlike what Simchoni did in his post, no conventional tests will be used to sort out the "significant" observations. Because the answer to the question that if the deep learning model can distinguish from patterns formed by chance and by nature is also interested. 

"A convnet takes as input tensors of shape (image height, image width, image channels)."[@DLR18] In this case, the format of all the images is $150\times150$ pixels, the convnet will be configured to process inputs of size (150, 150, 1).

180,000 data sets are generated for each of the two groups in the first experiment. 100,000 of them will be set apart as train set. In order to monitor during training the accuracy of the model on data it has never seen before, 40,000 of the rest of the data set will be used as the validation set. And the last 40,000 will become the unseen test set. 

10 epoches (1 epoch = 1 iteration over all samples) will be done for training. The model specification given by each epoch will be saved, the one gives the highest test power is chosen to represent the computers.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/whole_sample_plots.png}}
\caption{This graph shows the histogram of simulated n, histogram of simulated sigma, histogram of estimated p.value of t-test and the scatter plot of sigma against n. It shows the number of simulation is large enough to give good coverage.}
\label{expt2}
\end{figure*}

All convolutional neural network related work will be done by Keras package in R.
<!--
```{r dl_model, echo=FALSE, fig.cap="The deep learning model structure used in this thesis"}
model_loaded <- load_model_hdf5("data/weights.06-0.22.hdf5")
model_loaded
```
-->

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/new_plot.png}}
\caption{Training and validation metrics of the first experiment}
\label{expt2}
\end{figure*}

From the plot of the traning history, we can see the fourth specification gives the overall best performance with lowest loss and highest accuracy on validation set. Its accuracy on the unseen test set is also the best among other models. After the fourth checkpoint, the model starts to overfit.

```{r checkpoints, echo=F, paged.print=FALSE, results='asis'}
checkpoints <- c("4 epoch", "6 epoch", "10 epoch", "T-test")
powers <- c(0.8790, 0.8910, 0.8826, 0.9003)
alpha_1 <- c(0.9757, 0.9569, 0.9687, 0.9487) 
accuracy <- c(0.9273, 0.9239, 0.9257, 0.9245)
tibble(checkpoints, powers, alpha_1, accuracy) %>% kable(col.names = c("Tests", "Power", "1-Alpha", "Overall accuracy"), caption = "Performance of three checkpoints of the deep learning model on the unseen test set, where the power means the accuracy on the linear ones, (1-alpha) is the accuracy on the no-relationship ones. All numbers have been rounded to four decimal places.")
```

However, since the majority of the data plots in Turk's experiment have been generated with linear relationship, we will choose the sixth model here because it has the highest test power which provides the deep learning model a better chance. What's more, the $\alpha$ of the sixth model, which is the probability of Type I error, is the closest to 0.05. We can see the sixth model's performance is quite close to the conventional T-test.

## Comparing results

```{r linearresults, echo=F, paged.print=FALSE, results='asis'}
tests <- c("human", "computer", "t-test")
correct <- c(47, 44, 43)
accuracy <- c(0.6714, 0.6286, 0.6143)
tibble(tests, correct, accuracy) %>% kable(col.names = c("Tests", "No. of correct", "Accuracy"), caption = "Accuracy of testing the 70 data plots evaluated by human computer and the conventional t-test.")
```

The conclusion of human evaluation is calculated using the p-value calculation introduced in section 2 of @MM13, the null hypothesis is rejected if the calculated p-value is smaller than 0.05. 

The $\alpha$ of the conventional t-test we used in this comparison is 0.05. 

From this table, we can see human evaluation achieves the best accuracy on the Turk's experiment data set. Deep learning model and the conventional t-test perform similarly. 

## Aside discussion related to the comparing results
As we can see from the table, t-test and deep learning model behave quite similarly on both the test data set and Turk's experiment data. It is possible that the "Backpropagation algorithm" used in deep learning model is equivalent to maximum likelihood estimation in terms of linear regression problem in this case. Because all the error terms are following normal distribution as designed, they are also equivalent to the ordinary least square estimation. 
Since the fourth-epoch model behaves the best in all 10 models, and its estimated Type I error ($\alpha$) is not 0.05. We think the deep learning model may be in fact just finding the "best" $\alpha$ for t-test in this experiment, where "best" means giving the highest expected accuracy of the null and alternative hypothesis. 
To confirm this idea, a small simulation experiment was conducted to search for the "best" $\alpha$ for t-test. The data was simulated using the same specification as in the section "Linear relationship simulation" and "null plots simulation". The range of the $\alpha$ is from $(0.005,\ 0.010,\ 0.015,\ \dots,\ 0.100)$, with 200,000 iterations, the "best" $\alpha$ is found to be $\alpha=0.02$ which is very close to the one suggested by the fourth-epoch model $0.02435$.





