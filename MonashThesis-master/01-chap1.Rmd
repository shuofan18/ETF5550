---
chapter: 1
knit: "bookdown::render_book"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, cache=TRUE)
library(tidyverse)
library(nullabor)
```

# Introduction and literature review {#ch:intro}

<!--
Problem statement
-->
The multiple regression model for cross-sectional data is still the most widely used vehicle for empirical analysis in economics and other social sciences [@IE17]. Detecting possible violations of the Gauss-Markov assumptions is crucial to interpret the data properly, especially in the early stage of analysis. There are several distribution tests that are commonly used, for instance, the Breusch-Pagan test and White test for investigating heteroskedasticity. But primarily residual plots are the main diagnostic tool and these rely on human evaluation. Because plots show a lot more information than a single statistics. A good example here would be Anscome’s Quartet. It is a set of four distinct datasets each consisting of 11 (x,y) pairs where each dataset produces the same summary statistics (mean, standard deviation, and correlation) while producing vastly different plots [@ANS73]. Matejka and Fitzmaurice also did an interesting study on this issue, they used 'datasaurus' data from [@DS16] and generated a series of data with same statistics but very different plots [@JM17].


```{r saurus, fig.cap="Each dataset has the same summary statistics to two decimal places: (E(x)=54.26, E(y)= 47.83, sd(x) = 16.76, sd(y) = 26.93, Pearson’s r = -0.06)."}
library(datasauRus)
ggplot(datasaurus_dozen, aes(x = x, y = y, colour = dataset)) +
  geom_point() +
  theme_void() +
  theme(legend.position = "none") +
  facet_wrap( ~ dataset, ncol = 3)
```

<!--
Visual inference literature explanation - framework for assessing detection of difference from a null
-->

Former studies have shown that human eyes are sensitive to the systematic patterns in data plots. With proper manipulation, visualized plots can be used as test statistics and perform valid hypothesis test. A simple but rigorous protocol that provides inferential validity is lineup, named after the 'police lineup' of criminal investigations. The protocol consists of generating 19 null plots (could be other number), inserting the plot of the real data in a random location among the null plots and asking the human viewer to single out one of the 20 plots as most different from the others [@HW10]. If the real plot is chosen, it means the real data is different from the null hypothesis, so we reject the null hypothesis with 5% chance to be wrong (Type I error). Figure 1.2 is an example of lineup. Which plot do you think is the most different? If you choose one, you choose the plot of real data [@SIM18]. This protocol was proved to be valid and powerful therotically and also through human experiments, especially when the assumptions for doing conventional tests are violated [@MM13].

```{r lineup, message=FALSE, fig.cap="Scatterplot lineup example: one plot is the data, the rest are generated from a null model assuming no relationship between the two variables. In this lineup it is easy to see that plot 1, which is the data plot, is different from the rest.", fig.height=6, fig.width=6}
lineup_data_mtcars <- lineup(null_permute("hp"), mtcars, pos = 1)

ggplot(lineup_data_mtcars, aes(disp, hp)) +
  geom_point() +
  facet_wrap(~ .sample)
```

The question that arises today is whether we can train a computer to read residual plots, particularly with a computer vision approach such as deep learning. 

<!--
Motivation based on Simchoni's blog post
-->

Motivation for the task is provided in a blog post by Giora Simchoni [@SIM18]. He has designed a deep learning model to test the significance of linear relationship between two variables for samples of size 50. The model reached over 93% accuracy on unseen test data. He also mentioned that the computer fails to pick up a strong non-linear relationship even though the Pearson'r is as high as -0.84 [@SIM18]. So the short conclusion is the computer vision is not as flexible as human vision. As Simchoni explained in his article, the model can only tell linear relationship as trained. However, we think this fact is just another example reflecting the importance of visualization as we discussed above. Strong correlation does not necessarily mean linear relationship. We should always refer to the plot before making any statement. What's more, if we want the model to be more flexible, we could simply adjust our design of training accordingly. Therefore, in this article, we are trying to further Simchoni's study. More specifically, we want our model to perform two hypothesis tests as following. 

$H_0$: The residual plot belongs to a classical linear model where all assumptions are met.
$H_1$: There is heteroskedasticity in the data while other conditions are the same with the null.

$H_0$: The residual plot belongs to a classical linear model where all assumptions are met.
$H_1$: There is non-linear relationship in the data while other conditions are the same with the null.

<!--
Explain deep learning models 
-->

The model we will use is the convolutional neural networks, also known as convnets, a type of deep-learning model almost universally used in computer vision applications [@DLR18]. For simplicity, we can think of the deep learning neural network as a super complex nonlinear model which could estimate hundreds of thousands parameters with big enough dataset. As usual regression problem, to get the estimates of unknown parameters, we need to provide the model with dependent variable and independent variables. In this case, the independent variable will be the residual plots simulated from the null distribution and the alternative distribution, and dependent variable will be the labels of that plot indicating the true relationship of the original data. Once we have these estimated parameters, we then can use them to classify unseen residual plots, eg. to perform hypothesis tests. The structure of the model is given by figure 1.3. 

<!--
The layout of this chunk need to be adjusted
-->

```{r model, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

library(keras)
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu",
                input_shape = c(150, 150, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 3, activation = "softmax")

model %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

summary(model)
```

<!--
Scope of the experimental framework: non-linear relationship between dependent-independent variables 
-->





















