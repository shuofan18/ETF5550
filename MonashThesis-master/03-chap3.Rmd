---
chapter: 3
knit: "bookdown::render_book"
---

# New experiment comparing human vs. computer on reading heteroskedasticity

## Heteroskedasticity simulation

Linear model with heteroskedasticity is the model implied in the alternative hypothesis of the second experiment in this paper, where the constant variance assumption of the linear model is violated while all other conditions are met. 
By the definition given in @IE17, "The homoskedasticity states that the variance of the unobserved error, u, conditional on the explanatory variables, is constant. Homoskedasticity fails whenver the variance of the unobserved factors changes across different segments of the population, where the segments are determined by the different values of the explanatory variables."
There are countless types of heteroskedasticity since the "change of the variance" could be related to "the explanatory variables" in various ways. It is not feasible to list out all kinds of heteroskedasticity by a single function. For simplicity, we will focus on one example of them, a linear correlation between the explanatory variable X and the standard deviation of the error term. The conclusion from this experiment though can be generalized to more complicated cases.  
A new human experiment will be rendered to produce the human's performance on detecting heteroskedasticity by reading residual plots. After the deep learning model is trained on the data generated from this section, it will be tested on the same data set that is used for the human experiment. And the accuracy of the model will be compared to the human's performance from this new human experiment. To provide a reference to evaluate how computer perform in detecing heteroskedasticity issues, a special case of the White test will be used. Each image in test set will be evaluated using White test. The associated accuracy of White test will be compared to computers' accuracy on the same set.

The model structure is the same with the classic linear model:
$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$
with elements of the model generated by the following processes

- $X\sim U[-1,\ 1]$
To better present the heteroskedasticity in the data, a uniform distribution of X is used instead of normal distribution. The range is set to be small (from -1 to 1) in order to produce the data with weak heteroskedasticity more frequently.
- $\beta_0 = 0$
Intercept is set to be zero for the same reason stated above.
- $\beta_1\sim U[0.5,\ 1]$
$\beta_1$ has little impact in this case so it is set to be uniformly generated from 0.5 to 1. Because the residual plot but not data plot will be used here, therefore, the information contained in $\beta_1$ will be extracted by the linear regression we fit to the data.
- $\varepsilon\sim N(0,\ (aX+v)^2)$
The variance of the error term is a quadratic function of the explanatory variable which controls the magnitude of heteroskedasticity in the model.
- $a\sim U(-5,\ -0.05)\bigcup(0.05,\ 5)$
The parameter a here, following uniform distribution from -5 to 5 (excluding -0.05 to 0.05), is the correlation coefficient between X and the standard deviation. Larger a gives stronger heteroskedasticity. This range is wide enough for our purpose.
- $v \sim N(0,\ 1)$
This new error term is added to the variance of $\varepsilon$ so the relationship between the data can be more flexible.
- $n\sim U[20,\ 1500]$
The sample sizes will be randomly generated from 20 to 1500. We choose 20 to 1500, because when the sample size is smaller than 20, there is hardly any systematic patterns to see. And 1500 is large enough to give a good description about the true relationship within the data, and is light enough to be processed. Since we make the transparency of the points to be 0.4, more points being added to the plot will just make the plot looks darker. 

In general, the choice of the parameters is an empirical work. Primarily, we want the residual plots to show more variation; on the other hand, we need to limit the range of these parameters in order to keep the key features in the data. 

```{r heter_plot, message=FALSE, cache=TRUE, fig.cap="Four examples of residual plots generated from linear model with heteroskedasticity"}
heter<-function(i){
  
  n = sample(20:1500, 1)
  x <- runif(n, -1, 1)
  beta <- runif(1,0.5,1)
  a <- runif(1,0.05,4)*(-1)^(rbinom(1, 1, 0.5))
  sd <- a*x+rnorm(n, 0, 1)
  
  min <- min(sd)
  if (min<0){
    sd <- sd-min
  }
  
  y<-rnorm(n, beta*x, sd)
  df <- tibble(x, y)
  model<-lm(y ~ x, data=df)
  fit <- augment(model, df)
  heter_data <- fit %>% select(x, .std.resid)
  pic <- ggplot(heter_data, aes(x=x, y=.std.resid))+geom_point(alpha=0.4) + theme(aspect.ratio = 1)
  
}
set.seed(0518)
p1 <- heter(1)
p2 <- heter(2)
p3 <- heter(3)
p4 <- heter(4)
grid.arrange(p1,p2,p3,p4, nrow=2) 

```

## Human subject experiment

## Deep learning model training

For the second experiment, a linear model will be fit to the data firstly. Residuals from the fitted model will be standardized and extracted. The residual plot will be made of standardized residuals against X.

## Results


























