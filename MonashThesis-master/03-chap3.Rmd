---
chapter: 3
knit: "bookdown::render_book"
---

# New experiment comparing human vs. computer on reading heteroskedasticity

Turk experiment mainly considers linear data; we extend their study by including heteroscedasticity in this paper. A new database of human evaluating heteroskedasticity is created by a small experiment. This new database is used to compare the performance of the computer model, the convnets. The convnets is trained on the same parameter simulation framework and tested on the same data as the human evaluations.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/diag2exp.png}}
\caption{Human subject experiment set-up for the second hypothesis test. Four surveys were sent to 84 people, 22 participated, 218 effective evaluations were collected.}
\label{diag2exp}
\end{figure*}

## A new human experiment

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/heter_lineup_example.png}}
\caption{An example question in the survey, 4 out of 6 people picked the real plot, the real plot is the first one.}
\label{heterlu}
\end{figure*}

The experiment is to evaluate the human ability to read heteroskedasticity from residual plots. It is rendered at Monash University, Melbourne Australia. The participants are all students or lecturers in the department of econometrics and business statistics.

Four surveys were randomly sent to 84 people by email, three of the survey consists of ten lineup questions, and the fourth survey has only four lineup questions. All lineup questions are of size 20 plot and are designed to have different difficulty levels. Only one lineup question appears in the survey twice, thus, we have 33 ($10\times3+4-1$) distinct questions in total. A total number of 22 people have participated. Five people evaluated two surveys. One people selected four plots for each lineup by accident, this person's response was removed from the data. In summary, we collected 218 effective evaluations from 21 people. Figure \ref{heterlu} is an example of the lineup used as a question in the survey. The "real plot" and "null plot" data in all lineup questions was simulated using the same specifications given in the next two sections respectively.

## Heteroskedasticity simulation

A linear model with heteroskedasticity is the model implied in the alternative hypothesis of the second experiment in this paper, where the constant variance assumption of the linear model is violated while all other conditions are met. By the definition given in @IE17, "The homoskedasticity states that the variance of the unobserved error, u, conditional on the explanatory variables, is constant. Homoskedasticity fails whenever the variance of the unobserved factors changes across different segments of the population, where the segments are determined by the different values of the explanatory variables." There are countless types of heteroskedasticity since the "change of the variance" could be related to "the explanatory variables" in various ways. It is not feasible to list out all kinds of heteroskedasticity by a single function. For simplicity, we will focus on one example of them, a linear correlation between the explanatory variable X and the standard deviation of the error term. Hence the relationship between the explanatory variable X and the variance of the error term will be quadratic. The results of this experiment though can be generalized to more complicated cases.  

The model structure is the same with the classic linear model:
$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$
with elements of the model generated by the following processes

- $X\sim U[-1,\ 1]$
To better present the heteroskedasticity in the data, a uniform distribution of X is used instead of normal distribution. The range is set to be small (from -1 to 1) in order to balance the data with weak heteroskedasticity appearing more frequently.
- $\beta_0 = 0$
Intercept is set to be zero. Because the residual plot but not data plot is used in this experiment. Therefore, the information contained in $\beta_0$ will be extracted by the linear regression we fit to the data.
- $\beta_1\sim U[0.5,\ 1]$
$\beta_1$ has little impact in this case as well so it is set to be uniformly generated from 0.5 to 1. 
- $\varepsilon\sim N(0,\ (aX+v)^2)$
The variance of the error term is a quadratic function of the explanatory variable which controls the magnitude of heteroskedasticity in the model.
- $a\sim U(-5,\ -0.05)\bigcup(0.05,\ 5)$
The parameter a here, following uniform distribution from -5 to 5 (excluding -0.05 to 0.05), is the correlation coefficient between X and the standard deviation. Larger a gives stronger heteroskedasticity. This range is wide enough for our purpose.
- $v \sim N(0,\ 1)$
This new error term is added to the variance of $\varepsilon$ so the relationship between the data can be more flexible.
- $ax+v-min(ax+v)$ when $min(ax+v)<0$ 
To keep the simulated standard deviation positive, and to keep the structure of the relationship between X and the residuals, the $min(ax+v)$ is subtracted from $ax+v$ whenever the former is negative.
- $n\sim U[50,\ 500]$
The sample sizes are randomly generated from 50 to 500 to provide reasonable variations.

In general, the choice of the parameters is an empirical work. Primarily, we want the residual plots to show more variation; on the other hand, we need to limit the range of these parameters in order to keep the key features in the data. Figure \ref{fig:heterplot4} shows four examples of residual plots generated under this structure.

```{r heterplot4, message=FALSE, cache=TRUE, fig.cap="Four examples of residual plots generated from the linear model with heteroskedasticity"}
heter<-function(i){
  
  n = sample(50:500, 1)
  x <- runif(n, -1, 1)
  beta <- runif(1,0.5,1)
  a <- runif(1,0.05,4)*(-1)^(rbinom(1, 1, 0.5))
  sd <- a*x+rnorm(n, 0, 1)
  
  min <- min(sd)
  if (min<0){
    sd <- sd-min
  }
  
  y<-rnorm(n, beta*x, sd)
  df <- tibble(x, y)
  model<-lm(y ~ x, data=df)
  fit <- augment(model, df)
  heter_data <- fit %>% select(x, .std.resid)
  pic <- ggplot(heter_data, aes(x=x, y=.std.resid))+geom_point(alpha=0.4) + theme(aspect.ratio = 1)
  
}
set.seed(0518)
p1 <- heter(1)
p2 <- heter(2)
p3 <- heter(3)
p4 <- heter(4)
grid.arrange(p1,p2,p3,p4, nrow=2) 

```

## Null plot simulation

The null scenario in this experiment is the classic linear model. The model structure is the same as the heteroskedasticity one. When we simulate this data, we kept most of the parameters as the same with the alternative data and only changed the key feature of the error term. So the difference in this data set is:

- $\varepsilon\sim N(0,\ c)$

- $c=mean(ax+v)$

where $c$ is a constant which equals to the mean of the $ax+v$. All other parameters in the null data are the same as the heteroskedasticity data.

## White test

To provide a reference level of how computer and human are performing, a special case of the White test is used in this experiment. Every data set simulated from this section has been tested by this White test. The procedure of the White test [@IE17] is:

- Estimate OLS model for the data, obtain residuals ($\hat{u}$) and the fitted values ($\hat{y}$). Computer the squared OLS residuals ($\hat{u}^2$) and the squared fitted values ($\hat{y}^2$).

- Run an auxiliary regression as $\hat{u}^2=\eta_0+\eta_1  \hat{y}+\eta_2 \hat{y}^2+error$, obtain the R-squared $R_{\hat{u}^2}^2$.

- Calculate the LM statistic which follows $\chi_2^2$ distribution.

- Conclude based on p-values given certain $\alpha$.

We think this test is suitable for detecting the heterskedasticity specified in this section, because $\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_{i1}$, so the squared fitted value $\hat{y}^2$ contains both the squares of the independent variable $x^2$ and $x$ itself. This form is consistent with our model design where the variance of error term is a quadratic function of $x$.

## Computer model

In this experiment, a linear model first fit to the data. Residuals from the fitted model are standardized and extracted. The residual plot is made of standardized residuals against X. The convnets has the same structure as in the first experiment, and all hyper-parameters in this model are exactly the same as the previous one. Fifteen epochs are done in this experiment. All fifteen convnets models are saved. The training and validation metrics are shown in figure \ref{histheter}. The overfitting issue is again not severe in this case. The variation in accuracy and loss are very small even since the beginning. We believe the training data and the convnets are both large enough, and the model is well trained. Then the fourth, eleventh and fifteenth epoch models are selected to be tested on the test set and the accuracy is shown in table \ref{hetercheck}. The 11th model is chosen to represent computer for the competition according to the overall accuracy. It is also worth noting that the $\alpha$ (error rate in the null dataset) chosen by the convnets is between 1.6% to 3.3%, which is smaller than 5%; what's more, the power of the convnets (accuracy in the real dataset) is much higher than the white test. Therefore, the overall accuracy of the convnets is higher than the white test.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/heter_history_plot.png}}
\caption{Training and validation metrics of heteroskedasticity vs. null model in our second experiment}
\label{histheter}
\end{figure*}

\begin{table}[ht]
\centering
\begin{tabular}{rrlrl} \hline
 & Tests & Heter & Homo & Overall \\\hline
 & 4 epoch & 0.970 & 0.967 & 0.968 \\ 
 & 11 epoch & 0.963 & 0.984 & 0.974 \\ 
 & 15 epoch & 0.959 & 0.984 & 0.972 \\ 
 & 5\% White test & 0.856 & 0.952 & 0.904 \\\hline
\end{tabular}
\caption{Performance of three checkpoints from the convnets model, and the 5\% significant white-test, computed on the test set. Accuracy is reported for each class, and overall.} 
\label{hetercheck}
\end{table}

## Comparing results

The performance of computer and human are computed by the same methods stated in chapter 2. The final competition dataset is the twenty-seven "real plots" used in the four surveys. Figure \ref{humana} displays the relationship between the proportion of correct answers for each question against the value of the simulated "a" for the real plot in that lineup question. The proportion is increasing as the absolute value of "a" increases. This meets our expectation since the value of "a" controls the magnitude of "heteroskedasticity". The stronger the relationship is in the data, the more people are able to pick the real plot.

Table \ref{heteresult} presents the accuracy achieved by our convnets, human and the white test for testing heteroskedasticity in those twenty-seven real plots. Different from the first experiment, this time the convnets achieves the best performance against human and the white test. In addition, the accuracy achieved by the convnets is also much higher than its competitors. 

\begin{table}[ht]
\centering
\begin{tabular}{rrlrl}
  \hline
 & rank & tests & correct & accuracy \\\hline
 & 1.00 & Computer 2\% & 25.00 & 92.59\% \\ 
 & 2.00 & Human 5\% & 17.00 & 62.96\% \\ 
 & 2.00 & White-test 5\% & 17.00 & 62.96\% \\ 
 & 3.00 & White-test 2\% & 16.00 & 59.26\% \\ 
 & 4.00 & Human 2\% & 15.00 & 55.56\% \\ 
   \hline
\end{tabular}
\caption{Accuracy of testing the 27 data plots evaluated by human, computer and the conventional white-test.} 
\label{heteresult}
\end{table}

## Aside discussion

Our original design was to have three surveys of size ten questions and to include two "all-null" lineup questions per survey. By "all-null", it means the "real plot" in that lineup was also generated from the null distribution. People were expected to having difficulties picking the real plot in that situation. However, because of one mistake we made in the data simulation procedure, the variance of the error terms associated with the real plots in the all-null lineups was not a single constant but a series of random numbers. Even if this series of random numbers had no relationship with the independent variable $x$, they led to undesired patterns in the data such as non-linearity and extreme outliers. Hence many participants picked the real plot successfully for the six all-null lineup questions. To fix this issue, we had to remove the information related to the six all-null questions and made one extra survey to collect more data. This was the reason why we made three normal surveys plus an extra small one.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/human_against_a.png}}
\caption{Proportion of correct answers for each lineup question against the simulated correlation "a" from human evaluation}
\label{humana}
\end{figure*}















