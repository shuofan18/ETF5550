---
chapter: 3
knit: "bookdown::render_book"
---

# New experiment comparing human vs. computer on reading heteroskedasticity

Turk experiment mainly considers linear data, in this paper, we extend their study by including heteroscedasticity. A new database of human evaluations is created by a small experiment. This new database is used to compare the performance of the computer model. The computer is trained on the same parameter simulation framework, and tested on the same data as the human evaluations.

## Human experiment explanation

The experiment is to evaluate the human ability of reading heteroskedasticity from residual plots. It is rendered at Monash University, Melbourne Australia. The participants are all students or lecturers in this university.

Four survey are randomly sent to 84 people by email, three of the survey consist of ten lineup questions, and the fourth survey has only four lineup questions. Only one lineup question appears in the survey twice, thus, we have 33 ($10\times3+4-1$) distinct questions in total. A total number of 22 people have participated. Five people evaluated two surveys. One people selected four plots for each lineup, this person's response is removed from the data. In summary, we have 218 effective evaluations from 21 people. 

Figure \ref{heterlu} is an example of the lineup used as a question in the survey.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/heter_lineup_example.png}}
\caption{An example question in the survey, 4 out of 6 people picked the real plot, the real plot is the first one.}
\label{heterlu}
\end{figure*}

The "real plot" and "null plot" data is simulated using the same specifications given in the next two sections respectively.

## Heteroskedasticity simulation

Linear model with heteroskedasticity is the model implied in the alternative hypothesis of the second experiment in this paper, where the constant variance assumption of the linear model is violated while all other conditions are met. By the definition given in @IE17, "The homoskedasticity states that the variance of the unobserved error, u, conditional on the explanatory variables, is constant. Homoskedasticity fails whenver the variance of the unobserved factors changes across different segments of the population, where the segments are determined by the different values of the explanatory variables." There are countless types of heteroskedasticity since the "change of the variance" could be related to "the explanatory variables" in various ways. It is not feasible to list out all kinds of heteroskedasticity by a single function. For simplicity, we will focus on one example of them, a linear correlation between the explanatory variable X and the standard deviation of the error term. Hence the relationship between the explanatory variable X and the variance of the error term will be quadratic. The results from this experiment though can be generalized to more complicated cases.  

The model structure is the same with the classic linear model:
$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$
with elements of the model generated by the following processes

- $X\sim U[-1,\ 1]$
To better present the heteroskedasticity in the data, a uniform distribution of X is used instead of normal distribution. The range is set to be small (from -1 to 1) in order to balance the data with weak heteroskedasticity appearing more frequently.
- $\beta_0 = 0$
Intercept is set to be zero. Because the residual plot but not data plot is used in this experiment. Therefore, the information contained in $\beta_0$ will be extracted by the linear regression we fit to the data.
- $\beta_1\sim U[0.5,\ 1]$
$\beta_1$ has little impact in this case as well so it is set to be uniformly generated from 0.5 to 1. 
- $\varepsilon\sim N(0,\ (aX+v)^2)$
The variance of the error term is a quadratic function of the explanatory variable which controls the magnitude of heteroskedasticity in the model.
- $a\sim U(-5,\ -0.05)\bigcup(0.05,\ 5)$
The parameter a here, following uniform distribution from -5 to 5 (excluding -0.05 to 0.05), is the correlation coefficient between X and the standard deviation. Larger a gives stronger heteroskedasticity. This range is wide enough for our purpose.
- $v \sim N(0,\ 1)$
This new error term is added to the variance of $\varepsilon$ so the relationship between the data can be more flexible.
- $ax+v-min(ax+v)$ when $min(ax+v)<0$ 
To keep the simulated standard deviation positive, and to keep the structure of the relationship between X and the residuals, the $min(ax+v)$ is substracted from $ax+v$ whenever the former is negative.
- $n\sim U[50,\ 500]$
The sample sizes are randomly generated from 50 to 500 to provide reasonable variations.

In general, the choice of the parameters is an empirical work. Primarily, we want the residual plots to show more variation; on the other hand, we need to limit the range of these parameters in order to keep the key features in the data.

```{r heter_plot, message=FALSE, cache=TRUE, fig.cap="Four examples of residual plots generated from linear model with heteroskedasticity"}
heter<-function(i){
  
  n = sample(50:500, 1)
  x <- runif(n, -1, 1)
  beta <- runif(1,0.5,1)
  a <- runif(1,0.05,4)*(-1)^(rbinom(1, 1, 0.5))
  sd <- a*x+rnorm(n, 0, 1)
  
  min <- min(sd)
  if (min<0){
    sd <- sd-min
  }
  
  y<-rnorm(n, beta*x, sd)
  df <- tibble(x, y)
  model<-lm(y ~ x, data=df)
  fit <- augment(model, df)
  heter_data <- fit %>% select(x, .std.resid)
  pic <- ggplot(heter_data, aes(x=x, y=.std.resid))+geom_point(alpha=0.4) + theme(aspect.ratio = 1)
  
}
set.seed(0518)
p1 <- heter(1)
p2 <- heter(2)
p3 <- heter(3)
p4 <- heter(4)
grid.arrange(p1,p2,p3,p4, nrow=2) 

```

## Null plot simulation

The null scenario in this experiment is the classic linear model. The model structure is the same as the heteroskedasticity one. When we simulate this data, we kept most of the parameters as the same with the alternative data and only changed the key feature of the error term. So the difference in this data set is:

- $\varepsilon\sim N(0,\ c)$

- $c=mean(ax+v)$

$c$ is a constant which equals to the mean of the $ax+v$. All other parameters in the null data are the same with the heteroskedasticity data.

## White test

To provide a reference level of how computer and human perform, a special case of White test is used in this experiment. Every data set simulated from this section has been tested by the White test. The procedure of the White test [@IE17] is:

- Estimate OLS model for the data, obtain residuals ($\hat{u}$) and the fitted values ($\hat{y}$). Computer the squared OLS residuals ($\hat{u}^2$) and the squared fitted values ($\hat{y}^2$).

- Run an auxiliary regression as $\hat{u}^2=\eta_0+\eta_1  \hat{y}+\eta_2 \hat{y}^2+error$, obtain the R-squared $R_{\hat{u}^2}^2$

- Calculate the LM statistic which follows $\chi_2^2$ distribution

- Conclude based on p-values given certain $\alpha$

## Computer model

In this experiment, a linear model is fit to the data firstly. Residuals from the fitted model are standardized and extracted. The residual plot is made of standardized residuals against X. The model is still the convnets, and all hyper-parameter in this model are exactly the same as the previous one.

15 epoches are done in this section. All 15 convnets models are saved. The training and validation metrics are shown in figure \ref{histheter}.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/heter_history_plot.png}}
\caption{Training and validation metrics of heteroskedasticity vs. null model in our second experiment}
\label{histheter}
\end{figure*}

## Comparing results  

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/human_against_a.png}}
\caption{Proportion of correct answers for each lineup question against the simulated correlation "a" from human evaluation}
\label{humana}
\end{figure*}


```{r heteresults, echo=F, paged.print=FALSE, results='asis'}
tests <- c("Computer 2%", "Human 5%", "White-test 5%", "White-test 2%", "Human 2%")
rank <- c(1,2,2,3,4)
correct <- c(25, 17, 17, 16, 15)
accuracy <- c("92.59%","62.96%", "62.96%","59.26%","55.56%")
tibble(rank, tests, correct, accuracy) %>% kable(col.names = c("Rank","Tests", "No. of correct", "Accuracy"), caption = "Accuracy of testing the 27 data plots evaluated by human computer and the conventional white-test.")
```


```{r hetercompare, echo=F, fig.height=6, fig.width=8, fig.cap="P.values for each real plot given by human, computer and white-test respectively."}
library(reshape2)
survey <- read.csv("~/documents/github/etf5550/heter&linear/survey_ans_nonull.csv")
survey <- survey %>% mutate(wtpvalue=1-pchisq(wt.stat,2))
graphdata <- survey %>% select(form, question, pval, pcpvalue, wtpvalue) %>% 
  mutate(id=1:27)
colnames(graphdata) <- c("form", "question", "5% Human p.values",
                         "Computer prediction of Null", 
                         "5% White-test p.values", "ID")

graphdata <- graphdata %>% melt(id=c("form","question","ID"))

ggplot(graphdata, aes(x=ID, y=value, group=variable, color=variable))+
  geom_point()+
  geom_line()+
  facet_wrap(~variable, nrow = 3)+
  #xlab("Lineup Questions")
  scale_x_continuous(name="Lineup Questions", breaks=1:27, 
                   labels=c("1","2","3","4","5","6","7","8",
                            "9","10","11","12","13","14","15",
                            "16","17","18","19","20","21","22",
                            "23","24","25","26","27"))
#ggsave(filename = "~/documents/github/etf5550/presentation/figures/heter_pvalues_3test.png")
```




















