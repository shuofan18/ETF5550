---
chapter: 3
knit: "bookdown::render_book"
---

# New experiment comparing human vs. computer on reading heteroskedasticity

Turk experiment mainly considers linear data; we extend their study by including heteroscedasticity in this paper. A new database of human evaluations of heteroskedasticity is created by a small experiment. This new database is used to compare the performance of the computer model. The computer is trained on the same parameter simulation framework and tested on the same data as the human evaluations.

## Human experiment explanation

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/diag2exp.png}}
\caption{Human subject experiment set-up for the second hypothesis test. Four surveys were sent to 84 people, 22 participated, 218 effective evaluations were collected.}
\label{diag2exp}
\end{figure*}

The experiment is to evaluate the human ability to read heteroskedasticity from residual plots. It is rendered at Monash University, Melbourne Australia. The participants are all students or lecturers in the department of econometrics and business statistics.

Four surveys were randomly sent to 84 people by email, three of the survey consists of ten lineup questions, and the fourth survey has only four lineup questions. All lineup questions are of size 20 plot. Only one lineup question appears in the survey twice, thus, we have 33 ($10\times3+4-1$) distinct questions in total. A total number of 22 people have participated. Five people evaluated two surveys. One people selected four plots for each lineup by accident, this person's response was removed from the data. In summary, we collected 218 effective evaluations from 21 people. 

Figure \ref{heterlu} is an example of the lineup used as a question in the survey.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/heter_lineup_example.png}}
\caption{An example question in the survey, 4 out of 6 people picked the real plot, the real plot is the first one.}
\label{heterlu}
\end{figure*}

The "real plot" and "null plot" data in all lineup questions was simulated using the same specifications given in the next two sections respectively.

## Heteroskedasticity simulation

A linear model with heteroskedasticity is the model implied in the alternative hypothesis of the second experiment in this paper, where the constant variance assumption of the linear model is violated while all other conditions are met. By the definition given in @IE17, "The homoskedasticity states that the variance of the unobserved error, u, conditional on the explanatory variables, is constant. Homoskedasticity fails whenever the variance of the unobserved factors changes across different segments of the population, where the segments are determined by the different values of the explanatory variables." There are countless types of heteroskedasticity since the "change of the variance" could be related to "the explanatory variables" in various ways. It is not feasible to list out all kinds of heteroskedasticity by a single function. For simplicity, we will focus on one example of them, a linear correlation between the explanatory variable X and the standard deviation of the error term. Hence the relationship between the explanatory variable X and the variance of the error term will be quadratic. The results of this experiment though can be generalized to more complicated cases.  

The model structure is the same with the classic linear model:
$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$
with elements of the model generated by the following processes

- $X\sim U[-1,\ 1]$
To better present the heteroskedasticity in the data, a uniform distribution of X is used instead of normal distribution. The range is set to be small (from -1 to 1) in order to balance the data with weak heteroskedasticity appearing more frequently.
- $\beta_0 = 0$
Intercept is set to be zero. Because the residual plot but not data plot is used in this experiment. Therefore, the information contained in $\beta_0$ will be extracted by the linear regression we fit to the data.
- $\beta_1\sim U[0.5,\ 1]$
$\beta_1$ has little impact in this case as well so it is set to be uniformly generated from 0.5 to 1. 
- $\varepsilon\sim N(0,\ (aX+v)^2)$
The variance of the error term is a quadratic function of the explanatory variable which controls the magnitude of heteroskedasticity in the model.
- $a\sim U(-5,\ -0.05)\bigcup(0.05,\ 5)$
The parameter a here, following uniform distribution from -5 to 5 (excluding -0.05 to 0.05), is the correlation coefficient between X and the standard deviation. Larger a gives stronger heteroskedasticity. This range is wide enough for our purpose.
- $v \sim N(0,\ 1)$
This new error term is added to the variance of $\varepsilon$ so the relationship between the data can be more flexible.
- $ax+v-min(ax+v)$ when $min(ax+v)<0$ 
To keep the simulated standard deviation positive, and to keep the structure of the relationship between X and the residuals, the $min(ax+v)$ is subtracted from $ax+v$ whenever the former is negative.
- $n\sim U[50,\ 500]$
The sample sizes are randomly generated from 50 to 500 to provide reasonable variations.

In general, the choice of the parameters is an empirical work. Primarily, we want the residual plots to show more variation; on the other hand, we need to limit the range of these parameters in order to keep the key features in the data.

```{r heterplot4, message=FALSE, cache=TRUE, fig.cap="Four examples of residual plots generated from linear model with heteroskedasticity"}
heter<-function(i){
  
  n = sample(50:500, 1)
  x <- runif(n, -1, 1)
  beta <- runif(1,0.5,1)
  a <- runif(1,0.05,4)*(-1)^(rbinom(1, 1, 0.5))
  sd <- a*x+rnorm(n, 0, 1)
  
  min <- min(sd)
  if (min<0){
    sd <- sd-min
  }
  
  y<-rnorm(n, beta*x, sd)
  df <- tibble(x, y)
  model<-lm(y ~ x, data=df)
  fit <- augment(model, df)
  heter_data <- fit %>% select(x, .std.resid)
  pic <- ggplot(heter_data, aes(x=x, y=.std.resid))+geom_point(alpha=0.4) + theme(aspect.ratio = 1)
  
}
set.seed(0518)
p1 <- heter(1)
p2 <- heter(2)
p3 <- heter(3)
p4 <- heter(4)
grid.arrange(p1,p2,p3,p4, nrow=2) 

```

## Null plot simulation

The null scenario in this experiment is the classic linear model. The model structure is the same as the heteroskedasticity one. When we simulate this data, we kept most of the parameters as the same with the alternative data and only changed the key feature of the error term. So the difference in this data set is:

- $\varepsilon\sim N(0,\ c)$

- $c=mean(ax+v)$

$c$ is a constant which equals to the mean of the $ax+v$. All other parameters in the null data are the same as the heteroskedasticity data.

## White test

To provide a reference level of how computer and human are performing, a special case of the White test is used in this experiment. Every data set simulated from this section has been tested by the White test. The procedure of the White test [@IE17] is:

- Estimate OLS model for the data, obtain residuals ($\hat{u}$) and the fitted values ($\hat{y}$). Computer the squared OLS residuals ($\hat{u}^2$) and the squared fitted values ($\hat{y}^2$).

- Run an auxiliary regression as $\hat{u}^2=\eta_0+\eta_1  \hat{y}+\eta_2 \hat{y}^2+error$, obtain the R-squared $R_{\hat{u}^2}^2$.

- Calculate the LM statistic which follows $\chi_2^2$ distribution.

- Conclude based on p-values given certain $\alpha$.

## Computer model

In this experiment, a linear model first fit to the data. Residuals from the fitted model are standardized and extracted. The residual plot is made of standardized residuals against X. The convnets has the same structure as in the first experiment, and all hyper-parameters in this model are exactly the same as the previous one.

15 epochs are done in this section. All 15 convnets models are saved. The training and validation metrics are shown in figure \ref{histheter}. The overfitting issue is not severe in this case. The variation in accuracy and loss are very small since the beginning. We believe the training data and the convnets are large enough, and the model is well trained.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/heter_history_plot.png}}
\caption{Training and validation metrics of heteroskedasticity vs. null model in our second experiment}
\label{histheter}
\end{figure*}

The fourth, eleventh and fifteenth epoch models were selected to be tested on the test set and the accuracy is shown in table \ref{hetercheck}. The 11th model is chosen to represent computer in this section.

\begin{table}[ht]
\centering
\begin{tabular}{rrlrl} \hline
 & Tests & Heter & Homo & Overall \\\hline
 & 4 epoch & 0.970 & 0.967 & 0.968 \\ 
 & 11 epoch & 0.963 & 0.984 & 0.974 \\ 
 & 15 epoch & 0.959 & 0.984 & 0.972 \\ 
 & 5\% White test & 0.856 & 0.952 & 0.904 \\\hline
\end{tabular}
\caption{Performance of three checkpoints from the convnets model, and the 5\% significant white-test, computed on the test set. Accuracy is reported for each class, and overall.} 
\label{hetercheck}
\end{table}

## Comparing results

The performance of computer and human were computed by the same methods stated in chapter 2. Figure \ref{humana} displays the relationship between the proportion of correct answers for each question against the value of the simulated "a" for the real plot in that lineup question. The proportion is increasing as the absolute value of "a" increases. This meets our expectation since the value of "a" controls the magnitude of "heteroskedasticity". The stronger the relationship is in the data, the more people are able to pick the real plot.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/human_against_a.png}}
\caption{Proportion of correct answers for each lineup question against the simulated correlation "a" from human evaluation}
\label{humana}
\end{figure*}


\begin{table}[ht]
\centering
\begin{tabular}{rrlrl}
  \hline
 & rank & tests & correct & accuracy \\\hline
 & 1.00 & Computer 2\% & 25.00 & 92.59\% \\ 
 & 2.00 & Human 5\% & 17.00 & 62.96\% \\ 
 & 2.00 & White-test 5\% & 17.00 & 62.96\% \\ 
 & 3.00 & White-test 2\% & 16.00 & 59.26\% \\ 
 & 4.00 & Human 2\% & 15.00 & 55.56\% \\ 
   \hline
\end{tabular}
\caption{Accuracy of testing the 27 data plots evaluated by human, computer and the conventional white-test.} 
\label{heteresult}
\end{table}





















