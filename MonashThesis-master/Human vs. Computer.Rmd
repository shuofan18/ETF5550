---
title: 'Human vs. Computer: Can we teach the computer to read residual plots?'
degreetype: 'Master'
author: 'Shuofan Zhang'
degrees: 'Master, Monash University'
output: bookdown::pdf_book
site: bookdown::bookdown_site
link-citations: yes
knit: "bookdown::render_book"
---

<!-- 
Edit these lines as appropriate.
The actual thesis content is in several Rmd files.

You'll need to edit the _bookdown.yml file to set the order in which you'd like them to appear. 

If you have specific LaTeX packages to add, put them in monashthesis.tex.

You will need to ensure you have installed the knitr and bookdown packages for R.

You will also need LaTeX installed on your computer.
-->

<!--chapter:end:index.Rmd-->

---
knit: "bookdown::render_book"
---

# Acknowledgements {-}

I would like to thank my supervisor, Di, for being patient with me as always.

This thesis was written using R markdown with relevant code and data accessible with the text.

[https://github.com/shuofan18/ETF5550](https://github.com/shuofan18/ETF5550)

Software used to conduct this research is R [@R], Keras [@keras], ggplot2 [@ggplot2]

<!-- Tensorflow, tidyverse citation problem-->


<!--chapter:end:00-acknowledge.Rmd-->

---
knit: "bookdown::render_book"
---

# Declaration {-}  

I hereby declare that this thesis contains no material which has been accepted for the award of any other degree or diploma in any university or equivalent institution, and that, to the best of my knowledge and belief, this thesis contains no material previously published or written by another person, except where due reference is made in the text of the thesis.








\vspace*{2cm}\par\authorname

<!--chapter:end:00-declaration.Rmd-->

---
knit: "bookdown::render_book"
---

# Abstract {-}  

Residuals plots are a primary means to diagnose statistical models. It requires human evaluation to determine if structure in the plot is consistent with random variation or not. If not, then the diagnosis is that the model has not adequately captured the relationships between response and explanatory variable in the data. This thesis develops a computer vision model to read residual plots. It compares results with a large database of human evaluations. The evaluations were conducted using a protocol called the "lineup" which places residual plots in a formal framework for statistical hypothesis testing. The comparison between computer and human is made on a very restricted and controlled set of residual plot structures. A new small human subject study is also conducted to compare human vs. computer in reading heteroscedasticity.



\clearpage\pagenumbering{arabic}\setcounter{page}{0}


<!--chapter:end:00-abstract.Rmd-->

---
chapter: 1
knit: "bookdown::render_book"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, cache=TRUE)
library(tidyverse)
library(nullabor)
library(keras)
library(broom)
library(knitr)
```

# Introduction and literature review {#ch:intro}

"The multiple regression model for cross-sectional data is still the most widely used vehicle for empirical analysis in economics and other social sciences" [@IE17]. Detecting possible violations of the Gauss-Markov assumptions is crucial to interpret the data properly, especially in the early stage of analysis. There are several distribution tests that are commonly used, for instance, the Pearson correlation test for detecting linear relationship; the Breusch-Pagan test and White test for investigating heteroskedasticity. But primarily residual plots are the main diagnostic tool and these rely on human evaluation. Because data plots show a lot more information than a single statistic. A good example here would be Anscome’s Quartet. "It is a set of four distinct datasets each consisting of 11 (x,y) pairs where each dataset produces the same summary statistics (mean, standard deviation, and correlation) while producing vastly different plots" [@ANS73]. Matejka and Fitzmaurice also did an interesting study on this issue, they used 'datasaurus' data from @DS16 and generated a series of data with same statistics but very different plots [@JM17].

```{r saurus, fig.width=6, fig.height=7, fig.cap="Each dataset has the same summary statistics to two decimal places: (E(x)=54.26, E(y)=47.83, Pearson’s r=, sd(x)=16.76, sd(y)=26.93"}
library(datasauRus)
ggplot(datasaurus_dozen, aes(x = x, y = y, colour = dataset)) +
  geom_point() +
  theme_void() +
  theme(legend.position = "none") +
  #theme(aspect.ratio = 1) +
  facet_wrap( ~ dataset, ncol = 3)
```

## Lineup protocol  

Former studies have shown that human eyes are sensitive to the systematic patterns in data plots. With proper manipulation, visualized plots can be used as test statistics and perform valid hypothesis test. One example of these protocols that provides inferential validity is lineup which is introduced by @HW10. "The protocol consists of generating 19 null plots (could be other number), inserting the plot of the real data in a random location among the null plots and asking the human viewer to single out one of the 20 plots as most different from the others" [@HW10]. If the real plot is chosen, it means the real data is different from the null hypothesis, so we reject the null hypothesis with 5% chance to be wrong (Type I error). Figure 1.2 is an example of lineup. Which plot do you think is the most different? If you choose one, we are 95% confident to reject the no-relationship assumption between the two variables, hp and disp [@SIM18]. This protocol has proved valid and powerful theoretically as well as practically through human experiments, especially when the assumptions for doing conventional tests are violated [@MM13].

```{r lineup, message=FALSE, fig.cap="Scatterplot lineup example: one plot is the data, the rest are generated from a null model assuming no relationship between the two variables. In this lineup it is easy to see that plot 1, which is the data plot, is different from the rest.", fig.height=6, fig.width=6}
lineup_data_mtcars <- lineup(null_permute("hp"), mtcars, pos = 1)

ggplot(lineup_data_mtcars, aes(disp, hp)) +
  geom_point() +
  theme(aspect.ratio = 1) +
  facet_wrap(~ .sample)
```

The question that arises today is whether we can train a computer to read scatter plots, particularly with a computer vision approach such as deep learning. 

## Computer vision  

Motivation for the task is provided in a blog post by Giora Simchoni [@SIM18]. He has designed a deep learning model to test the significance of linear relationship between two variables for samples of size 50. The model reached over 93% accuracy on unseen test data. He also mentioned that the computer fails to pick up a strong non-linear relationship even though the Pearson'r is as high as -0.84 [@SIM18]. So the short conclusion is the computer vision is not perfect, in that it is not as flexible as human vision. As Simchoni explained in his article, the model can only distinguish linear relationship from no-relationship as trained. However, we think this fact is just another example reflecting the importance of visualization as we discussed above. Strong correlation does not necessarily mean linear relationship. We should always refer to the plot before making any statement. What's more, if we want the model to be more flexible, we could simply adjust our design of training accordingly. Therefore, in this article, we are trying to further Simchoni's study. More specifically, we will build a computer model to perform two hypothesis tests as following. 

$H_0$: There are no relationships between the two variables.

$H_1$: There is linear relationship between the two variables where all Gauss-Markov assumptions are met.


$H_0$: There is linear relationship between the two variables where all Gauss-Markov assumptions are met.

$H_1$: There is linear relationship between the two variables where the variance of the error term is not a constant while all other Gauss-Markov assumptions are met.

For ease of exposition, only regression model with one explanatory variable will be considered in this paper, but many of the results can be generalized to other cases including multiple regression model. Because the "statistics" we will use is scatter plot, in terms of teaching the computer of reading the plot, one variable is enough to generate different patterns in that plot for convnets to learn. And this makes the design process much simpler. 

The model we will use is the convolutional neural networks, also known as convnets, a type of deep-learning model "almost universally used in computer vision applications" [@DLR18]. Unlike the classical programming where human input rules, in deep learning paradigm, we provide data and the answers associated with the data. Deep learning algorithm will output the rules, and these rules can then be used on new data to make predictions.
We can also think of the deep learning neural network as a complex nonlinear model which could estimate millions of parameters ($\textbf{w}$) with big enough dataset. As usual regression problem, to get the estimates of unknown parameters ($\textbf{w}$), we need to provide the model with dependent variable ($y_i$) and independent variables ($\textbf{x}_i$). In this case, the independent variable will be the images of data plots (in forms of matrices) simulated from the null distribution and the alternative distribution, and dependent variable will be the labels of that plot indicating the true relationship of the original data. Once we have the estimated parameters ($\hat{\textbf{w}}$), we then can use them to classify unseen data plots, eg. to perform hypothesis tests.
The estimation method for deep-learning model is called Backpropagation algorithm which "is a way to train chains of parametric operations using gradient-descent optimization". [@DLR18] The gradient-descent optimizer is meant to find the set of parameters such that the cost function reaches its minimum. The form of the cost functions or loss function, should be determined according to each question.
In both of the two experiments conducted in this paper, the deep learning model is expected to complete binary classification task, eg. tell "linearly correlated" variables from "independent" variables for the first experiment, tell "heteroskedasticity errors" from "normal errors" for the second experiment. 
"Crossentropy is usually the best choice (as the loss function) when you’re dealing with models that output probabilities" as introduced by @DLR18. Originated from Information Theory, Crossentropy is a quantity measuring the distance between probability distributions. In deep learning world, it measures the distance between the true distribution and the predictions. 
Therefore, in this paper, the binary crossentropy loss function will be used. The associated cost function is of the form,
$$J(\textbf{w})=- \frac{1}{N}\sum_{i=1}^N\left(  
\ y_i\ log\hat{y_i}+(1-y_i)\ log(1-\hat{y_i})  
\right)$$
where $\hat{y_i}  =  g(\textbf{w} \times \textbf{x}_i)  = \frac{1}{1+e^{-\textbf{w} \times \textbf{x}_i}}$ and $g(z)$ is the logistic function.

## Comparing human vs. computer  

Chapter 2 will compare computer performanace against database of human evaluation in reading linear relationship. Steps of constructing computer experiment will be discussed, Turk's study will be explained, the comparison results will be given. Chapter 3 will compare computer performance against the results from the new human subject study. Details of this new human subject study will be provided. The results will also be presented.







<!--chapter:end:01-chap1.Rmd-->

---
chapter: 2
knit: "bookdown::render_book"
---

# Comparing computer performance against database of human evaluation

A database of human evaluations of scatterplots of residuals against fitted is available from prior studies. This is used to compare the performance of the computer model. The computer model is trained on a broader parameter simulation framework, and tested on the same data as the human evaluations. 

## Amazon Mechanical Turk study explanation

A large database of results from a human subjects was collected examine the performance of the lineup protocol relative to a classical tests. The work is published in @MM13. This database forms the basis of the test set used to examine the computer model performance. 

In @MM13, "three experiments were conducted to evaluate the effectiveness of the lineup protocol relative to the equivalent test statistic used in the regression setting." In each experiment, they simulated data from a controlled setting and then generated associated lineup for human to evaluate. The human subjects were hired from Amazon Mechanical Turk where is a a marketplace for work that requires human intelligence.

The controlled model in their first experiment is 
$$Y_i=\beta_0+\beta_1 X_{i1}+\beta_2 X_{i2}+\epsilon_i$$ where $\beta_0=5, \beta_1=15, X_1 \sim Poisson(\lambda=30), \epsilon_i\sim N(0,\sigma^2), i=1,2,...,n$. While in the null model $\beta_2=0$, and the null data was generated by simulating from $N(0,\hat{\sigma}^2)$. This experiment was aimed to test the ability of human on detecting the effect of $X_2$.

Their second experiment is very similar to the first one, but there is only one continuous variable $X_1$ on the right hand side. The actual data model is
$$Y_i=\beta_0+\beta_1 X_{i1}+\epsilon_i$$ where $\beta_0=6, X_1\sim N(0,1)$, and the null data was generated from $N(X\hat{\beta}, \hat{\sigma}^2)$.

The third experiment in their paper contains contaminated data where the actual data were in fact generated from two different specifications. 
$$Y_i=
  \begin{cases}
    \alpha+\beta X_i+\epsilon_i       & \quad X_i\sim N(0,1)\ \ i=1,...,n\\
    \lambda+\eta_i  & \quad X_i\sim N(\mu,1/3)\ \ i=1,...,n_c
  \end{cases}
$$ where $\epsilon_i \sim N(0,\sigma), \eta_i \sim N(0,\sigma/3), \ \mu=-1.75, \ \beta\in(0.1, 0.4, 0.75, 1.25, 1.5, 2.25)$. And $n=100, n_c=15, alpha=0, \lambda=10, \sigma=3.5$. The null plots were generated from $N(0,\hat{\sigma}^2)$.

Other parameters in the "actual data sets" of experiment one and two are shown in the table below. 

```{r turktable,ref.label='turktable', echo=F, results='asis'}
c1 <- c(100,100,300,300)
c2 <- c(5,12,5,12)
c3 <- c("0,1,3,5,8", "1,3,8,10,16", "0,1,2,3,5", "1,3,5,7,10")
c4 <- c("0.25, 0.75, 1.25, 1.75, 2.75", "0.5, 1.5, 3.5, 4.5, 6", "0.1, 0.4, 0.7, 1, 1.5","0, 0.8, 1.75, 2.3, 3.5")
tibble(c1,c2,c3,c4) %>% kable(col.names = c("Sample size (n)", "Error SD(sigma)", "Experiment 1 beta2", "Experiment 2 beta1"), align = "c", caption = "Combination of parameter values used for simulation in Turk's study.")
```

Their experiment 2 examined the performance of humans in recognising linear association between two variables, in direct comparison to conducting a $t$-test of $H_o: \beta_k=0$ vs $H_a: \beta_k\neq 0$ assessing the importance of including variable $k$ in the linear model. An example lineup is shown in Figure \ref{expt2}. For this lineup, 63 of the 65 people who examined it selected the data plot (position 20) from the null plots. There is clear evidence that the data displayed in plot 20 is not from $H_o: \beta_k=0$. 

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/plot_turk2_300_350_12_3.png}}
\caption{One of 70 lineups used in experiment 2 Majumder et al (2012). Of the 65 people who examined the lineup,  63 selected the data plot, which is in position 20.}
\label{expt2}
\end{figure*}

This experiment 2 utilised 70 lineups of size 20 plot, with varying degrees of departure from the $H_o: \beta_k=0$. There were 351 evaluations by human subjects. These results will be used for comparison with the deep learning model. 

## Linear relationship simulation

The design for this model is similar to what @SIM18 did in his blog but is tailored to compare the computer performance with the Turk study results.

The model is designed as:
$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$
which is the same with the data generating model of Turk's experiment 2. And all the parameters in our model will be designed to cover the range used in the second experiment in @MM13. The relevant parameters are generated using the following specification.

- $X \sim N[0,\ 1]$  
Distributions of X has impact on the shape of the scatters. For instance, if X is generated from a uniform distribution, then the plots will look like a square especially when the sample size is large; while more like a circle if X follows normal distribution. 

- $\beta_0 = 0$  
Intercept is set to be zero, because it will not change the patterns in the data plots.

- $\beta_1\sim U[-10, -0.1] \bigcup [0.1, 10]$  
$\beta_1$ is designed to be uniformly generated from -10 to 10 (excluding -0.1 to 0.1).

- $\varepsilon\sim N(0, \sigma^2) \ where\ \sigma \sim U[1,12]$  
$\varepsilon$ is designed to be uniformly generated from 1 to 12.

- $n=U[50,500]$  
The sample sizes of each data set vary from 50 to 500.

Figure \ref{fig:linear} shows four example plots generated using the specifications above. Under this controlled structure, a total number of 240,000 data sets are simulated. The histograms of the simulated $n, \beta, \sigma$, the estimated p value and the scatter plots of $\beta$ against n, $\sigma$ against n in figure \ref{fig:simplot} show good coverage over all the values.

```{r linear, fig.height=6, fig.width=6, echo=FALSE, fig.cap="Four examples of data plots generated from the classic linear model."}
library(gridExtra)
linear<-function(n){
   beta <- sample(c((-10:-0.1),(0.1:10)), 1)
  
  sigma <- sample(1:12 , 1)
  x<-rnorm(n, 0, 1)
  y<-rnorm(n, beta*x, sigma)
  
  tibble(x, y) %>% 
  ggplot(aes(x = x, y = y)) + 
    geom_point(alpha = 0.4) +
    theme(aspect.ratio = 1)
}
set.seed(0420)
n1 <- sample(c(100, 300), 1)
n2 <- sample(c(100, 300), 1)
n3 <- sample(c(100, 300), 1)
n4 <- sample(c(100, 300), 1)

p1 <- linear(n1)
p2 <- linear(n2)
p3 <- linear(n3)
p4 <- linear(n4)

grid.arrange(p1,p2,p3,p4,nrow=2, heights=c(1,1)) 

```


```{r simplot, fig.height=8, fig.width=6, fig.cap="Overview of parameter values used in the linear class simulation, for computer model training. Good coverage is obtained across the parameter space."}
parameters_b <- read.csv("data/parameters_b.csv")
hist_n_l <- ggplot(data = parameters_b, aes(n)) + geom_histogram(binwidth = 50, center = 25) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated n")

hist_b_l <- ggplot(data = parameters_b, aes(beta)) + geom_histogram(binwidth = 1, center=-10.5) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated beta from linear model")

hist_p_l <- ggplot(data = parameters_b, aes(ct.p.value)) + geom_histogram(binwidth = 0.1) +
  xlab("p-value") + 
  theme(plot.title = element_text(size = 8)) + 
  ggtitle("Histogram of estimated p value of t-test from linear model")

hist_s_l <- ggplot(data = parameters_b, aes(sigma)) + geom_histogram(binwidth = 1, center=-0.5) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated sigma from linear model")

scap_beta_n <- ggplot(data = parameters_b, aes(x=n, y=beta)) + geom_point(shape=".") + ggtitle("Scatter plot of simulated n and beta from linear model") + theme(plot.title = element_text(size = 8))

scap_sigma_n <- ggplot(data = parameters_b, aes(x=n,y=sigma)) + geom_point(shape=".") + ggtitle("Scatter plot of simulated n and sigma from linear model") + theme(plot.title = element_text(size = 8))

grid.arrange(hist_n_l, hist_b_l, hist_s_l, hist_p_l, scap_beta_n, scap_sigma_n, ncol=2)
```

## Null plot simulation

This is the null scenario in our first experiment, eg. the two variables under tested are independent of each other. If the data arises from this situation, then the data plots will not show any systematic patterns theoretically. 

The model is designed the same as the linear model:
$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$
with elements of the model generated using the same specification as the linear model, except  

- $\beta_1 = 0$  

The coefficient of $X_i$ is always zero. So $X_i$ and $Y_i$ are uncorrelated of each other.

Figure \ref{fig:norela} are four example plots generated using the specifications above. Same as the linear model simulation, a total number of 240,000 data sets are simulated under this structure. The histograms of the simulated $n, \beta, \sigma$, the estimated p value and the scatter plots of $\beta$ against n, $\sigma$ against n in figure \ref{fig:simplot2} show good coverage over all the values.

```{r norela, echo=F, fig.height=6, fig.width=6, fig.cap="Four examples of data plots generated with two independent variables"}
norela<-function(i){
  n <- sample(c(100, 300), 1)
  beta <- 0
  sigma <- sample(1:12 , 1)
  x<-rnorm(n, 0, 1)
  y<-rnorm(n, beta*x, sigma)
  pic <- tibble(x, y) %>% 
    ggplot(aes(x = x, y=y)) + 
    geom_point(alpha = 0.4) +
    theme(aspect.ratio = 1)
}
set.seed(0518)
p1 <- norela(1)
p2 <- norela(2)
p3 <- norela(3)
p4 <- norela(4)
grid.arrange(p1,p2,p3,p4, nrow=2)
```

```{r simplot2, fig.height=6, fig.width=6, fig.cap="Overview of parameter values used in the null class simulation, for computer model training. Good coverage is obtained across the parameter space."}
parameters_0 <- read.csv("data/parameters_0.csv")
hist_n_0 <- ggplot(data = parameters_0, aes(n)) + geom_histogram(binwidth = 50, center = 25) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated n from null model")

hist_p_0 <- ggplot(data = parameters_0, aes(ct.p.value)) + geom_histogram(binwidth = 0.1, center=0.05) +
  xlab("p-value") + 
  theme(plot.title = element_text(size = 8)) + 
  ggtitle("Histogram of estimated p value of t-test from null model")

hist_s_0 <- ggplot(data = parameters_0, aes(sigma)) + geom_histogram(binwidth = 1, center=-0.5) + theme(plot.title = element_text(size = 8)) + ggtitle("Histogram of simulated sigma from null model")

scap_sigma_n0 <- ggplot(data = parameters_0, aes(x=n,y=sigma)) + geom_point(shape=".") + ggtitle("Scatter plot of simulated n and sigma from null model") + theme(plot.title = element_text(size = 8))

grid.arrange(hist_n_0, hist_s_0, hist_p_0, scap_sigma_n0, ncol=2)
```

```{r simplot2, fig.height=6, fig.width=6, fig.cap="Overview of parameter values used in the null class simulation, for computer model training. Good coverage is obtained across the parameter space."}
parameters_0 <- read.csv("data/parameters_0.csv")
hist_n_0 <- ggplot(data = parameters_0, aes(n)) + geom_histogram(binwidth = 50, center = 25) + theme(plot.title = element_text(size = 20)) + ggtitle("Histogram of simulated n")

hist_p_0 <- ggplot(data = parameters_0, aes(ct.p.value)) + geom_histogram(binwidth = 0.1, center=0.05) +
  xlab("p-value") + 
  theme(plot.title = element_text(size = 20)) + 
  ggtitle("Histogram of estimated p value of t-test")

hist_s_0 <- ggplot(data = parameters_0, aes(sigma)) + geom_histogram(binwidth = 1, center=-0.5) + theme(plot.title = element_text(size = 20)) + ggtitle("Histogram of simulated sigma")

scap_sigma_n0 <- ggplot(data = parameters_0, aes(x=n,y=sigma)) + geom_point(shape=".") + ggtitle("Scatter plot of simulated n and sigma") + theme(plot.title = element_text(size = 20))

histnull <- grid.arrange(hist_n_0, hist_s_0, hist_p_0, scap_sigma_n0, ncol=2)
ggsave(histnull, filename = "~/documents/etf5550/presentation/figures/histnull.png")
```

All simulated data and associated parameters including estimated sample p-values of t-test are saved and are used later on for calculating the performance of conventional t-test. 

## Computer model  

Convolutional neural networks have been developed primarily for classifying images. It is the primary clasification model used in computer vision. It has two interesting properties: "the patterns they learn are translation invariant", and "they can learn spatial hierarchies of patterns" [@DLR18]. The first one implies that once the model learns how to recognize linear patterns, it can be detected regardless of the direction, thus handling different slopes. 

All convolutional neural network modeling is done by the Keras [@keras] package in R, which interfaces to the python software. The plots used for training and testing in this section is the scatter plot between the dependent variable Y and the independent variable X. The R package @ggplot2 is used to generate the plots. All plots are saved as png and will be resized to have width and height both equal to 150 pixels. This size is similar to the plot size used in the lineup for human evaluation. As for the labels given to each image, we use the true population as the samples' identification directly. It is true that there will be some undesired patterns formed out of randomness, especially when the sample size is small. Unlike what Simchoni did in his post, no conventional tests will be used to sort out the "significant" observations. Because the answer to the question that if the deep learning model can distinguish from patterns formed by chance and by nature is also interested. 

As mentioned above, 240,000 data sets are generated for each of the two groups in the first experiment. 100,000 of them are set apart for training. Another 40,000 of the data sets are set apart as validation set in order to monitor during training the accuracy of the model on data it has never seen before. And the leftover (100,000 data sets) become the unseen test set. We make the test set so large that we can compare the performance of the convnet with the conventional t-test properly.

"A convnet takes as input tensors of shape (image height, image width, image channels)."[@DLR18] The channels are normally equal to three for RGB. In our case, the input tensors are of shape $150 \times 150 \times 1$ because they are grayscale images. Therefore the convnet will be configured to process inputs of size (150, 150, 1). We’ll do this by passing the argument input_shape = c(28, 28, 1) to the first layer.

```{r modelstructure, echo=TRUE, fig.height=6, results='none', warning=FALSE}
library(keras)
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), 
                activation = "relu",
                input_shape = c(150, 150, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), 
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), 
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), 
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
model
```

As shown in the table above, the "conv" and "pooling" working together slided the data from $150 \times 150 \times 1$ to $7 \times 7 \times 128$ (3D output). The figure \ref{diagconv} decribes this transformation. 

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/diagconv.png}}
\caption{Illustration of convolution and pooling steps on an image. The convolution step applies a fixed number of filters to sliding windows of $3\times 3$ cells. Pooling applies a statistic to distinct $2\times 2$ tiling of the image. In our model, the statistic used is the maximum of the four values. These transformations are the pre-processing steps done on every image in the training sample, to fit the model, and also to the validation and test images prior to prediction. }
\label{diagconv}
\end{figure*}

Then we need to flatten these 3D tensor into 1D tensor so that they can be processed by the "sigmoid" function in the end. The "sigmoid" is in fact a special case of logistic function. $S(x)=\frac{1}{1+e^{-x}}$. This function is the same one used to predict $\hat{y_i}$ and to calculate the cost function.

From the model structrue we can see that a total number of 3,452,545 parameters need to be estimated, this is done by gradient descent. 10 epoches (1 epoch = 1 iteration over all samples) are done for training. The model specification given by each epoch is saved, the one gives the overall best performance is chosen to represent the computers. Because the plot of the training history (figure \ref{histlinear}) shows overfitting starts from the fourth epoch, the values of accuracy and loss from validation set are very close after the fourth epoch.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/linear_history_plot.png}}
\caption{Training and validation metrics of linear vs. null model in our first experiment}
\label{histlinear}
\end{figure*}

Hence, we select the fourth, sixth, eighth and the tenth model to have them tested on the unseen test set. And the results are shown in the table below. The "$1-\alpha$" is the accuracy of each computer model tested on the "null data" in test set only. $\alpha$ here is an analogy to the Type I error in the conventional hypothesis test. Similarly, the "power" is the accuracy of each computer model tested on the "linear data" in the test set only. The t-test performance is calculated under 5% significance level. 

<!-- table display is weird

```{r checkpoints, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
checkpoints <- c("4 epoch", "6 epoch", "8 epoch", "10 epoch","5% T-test")
powers <- c(0.8916, 0.8885, 0.8958, 0.9042, 0.9206)
alpha_1 <- c(0.9842, 0.9858, 0.9812, 0.9712, 0.9489) 
accuracy <- c(0.9379, 0.9372, 0.9385, 0.9377, 0.9348)
tbl <- tibble(checkpoints, powers, alpha_1, accuracy) 
kable(tbl, col.names = c("Tests", "Power", "1-Alpha", "Overall accuracy"), caption = "Performance of four checkpoints from the convnets as well as 5% significant t-test tested on the unseen test set, where the power means the accuracy on the linear ones, (1-alpha) is the accuracy on the no-relationship ones. All numbers have been rounded to four decimal places.")
```
-->
\begin{table}[ht]
\begin{center}
\begin{tabular}{|l|rrr|}\hline
Tests & Linear & Null & Overall \\\hline
4 epoch & 0.892 & 0.984 &  0.938 \\   
6 epoch & 0.889 & 0.986 & 0.937 \\
8 epoch & 0.896 & 0.981 & 0.939 \\
10 epoch & 0.904 & 0.971 & 0.938 \\
5\% $t$-test & 0.921 & 0.949 & 0.935\\\hline
\end{tabular}
\end{center}
\caption{Performance of four checkpoints from the {\em convnets} model, and the 5\% significant $t$-test, computed on the test set. Accuracy is reported for each class, and overall. There is a slight improvement as the number of epochs increases, with 10 epochs being reasonably close to the ideal $t$-test accuracy.}
\label{checkpoints}
\end{table}

Since the test set is large enough (200,000 in total) to provide reliable reference. The 8th model is chosen according to the overall accuracy on the test set.
 
We should note that since the majority of the data plots in Turk's experiment have been generated with linear relationship (when the alternative hypothesis is true), it is a disadvantage for the computer comparing in terms of being tested on the Turk's data. Because of the difference in $\alpha$ ($\alpha \approx 0.02$ for the 8th computer model) the 5% significant t-test and 5% human evaluations may have higher power than the computer model.

## Comparing results  

First compare the experiment process for human and computer respectively by two diagrams figure \ref{dgpc} and figure \ref{dghm}.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/diagpc.png}}
\caption{Diagram illustrating the training, diagnosis and choice of the computer model. Based on 480,000 simulated data sets used to create $150\times 150$ pixel images, divided into training, validation and test sets.}
\label{dgpc}
\end{figure*}

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/diaghm.png}}
\caption{Diagram illustrating the process of human subject evaluation of lineups, and how performance is computed.}
\label{dghm}
\end{figure*}

The performance of the computer model for the Turk study data is tested in three steps:

- Re-generate the 70 "real plots" using the same data in Turk study (without null plots);

- Create a seperate test directory for the 70 "real plots" exclusively;

- The computer model's predicted accuracy over the 70 "real plots" are recorded as the model's performance.

The conclusion of human evaluation is obtained differently from the computer's. Because human evaluated "lineup", not only the "real plots". The performance is tested in five steps:

- Count total number of evaluations made by human for one lineup (N) and the number of correct answers for that lineup (k);

- Obtain N and k for all 70 lineup;

- Calculate p-value associated with each real plot using the formula introduced in section 2 of @MM13;

- Draw conclusion: reject the null when the calculated p-value is smaller than $\alpha$.

- The accuracy of the conclusions the 70 real plots is presenting for the human performance.

For a fair competition, the Type I error ($\alpha$) should be held the same for all test methods. However, we do not have direct control over the $\alpha$ of the computer model. Therefore, 2% significant t-test and 2% significant human conclusion is also included to give a complete picture of the comparison. 

```{r linearresults, echo=F, paged.print=FALSE, results='asis'}
tests <- c("Human 5%", "Human 2%", "T-test 5%", "Computer 2%", "T-test 2%")
rank <- c(1,1,2,3,4)
correct <- c(47, 47, 43, 39, 39)
accuracy <- c(0.6714,0.6714, 0.6143, 0.5571, 0.5571)
tibble(rank, tests, correct, accuracy) %>% kable(col.names = c("Rank","Tests", "No. of correct", "Accuracy"), caption = "Accuracy of testing the 70 data plots evaluated by human computer and the conventional t-test.")
```

The comparing result is interesting. Human achieves the highest accuracy, and the conclusion from human evaluation is robust to smaller p-values; 5% significant t-test is the second best, 2% significant t-test and the computer model perform similarly.

## Aside discussion related to the comparing results

As we can see from the two performance tables above, t-test and convnets behave quite similarly on both the test data set and Turk's experiment data. 

```{r ttdl, echo=FALSE, fig.height=3, fig.width=5, fig.align='center', fig.cap="Comparison between computer model and t-test for alpha in (0.01, 0.04), they perform very similarly, but t-test has overall better performance."}

tt <- read.csv("~/documents/github/etf5550/linear&norela/ttest_test_set.csv")
tt <- select(tt, alpha, power, accuracy)
tt <- filter(tt, alpha<0.04)
tt <- filter(tt, alpha>0.01)
colnames(tt) <- c("alpha", "power", "accuracy")
tt <- tt %>% mutate(model="T-test")
dl <- read.csv("~/documents/github/etf5550/linear&norela/test_acc_10epoches.csv")
dl <- dl %>% mutate(alpha=1-alphas)
dl <- select(dl, alpha, powers, accuracy) 
colnames(dl) <- c("alpha", "power", "accuracy")
dl <- dl %>% mutate(model="Deep learning model")

tt_dl <- rbind(tt,dl)

library(reshape2)
tt_dl <- tt_dl %>% melt(id=c("model","alpha"))

ggplot(tt_dl, aes(x=alpha, y=value, group=variable, color=model))+
  geom_point(size=3)+
  geom_smooth(method = lm, formula = y ~ splines::bs(x, 3), se = FALSE)
```

It is possible that the convnets is in fact doing the same thing as t-test in this case. Or the strategy it learned in this case turns out to be t-test. 

To confirm this idea, we calculated the accuracy of t-test again, with different $\alpha$ (from 0.005 to 0.1 with 0.005 increments) on all 200,000 test sets. The estimated power and overall accuracy were recorded. When $\alpha=0.015$, the overall accuracy reaches its maximum. This value approximately coincide with the $\alpha$ chosen by the convnets. And since the $\alpha$ of convnets is from 0.0142 to 0.0347 in this case, we truncated the t-test data as well for figure \ref{fig:ttdl}. From this graph, we can see the two tests perform very similarly, but t-test has overall better performance.






<!--chapter:end:02-chap2.Rmd-->

---
chapter: 3
knit: "bookdown::render_book"
---

# New experiment comparing human vs. computer on reading heteroskedasticity

Turk experiment mainly considers linear data, in this paper, we extend their study by including heteroscedasticity. A new database of human evaluations is created by a small experiment. This new database is used to compare the performance of the computer model. The computer is trained on the same parameter simulation framework, and tested on the same data as the human evaluations.

## Human experiment explanation

The experiment is to evaluate the human ability of reading heteroskedasticity from residual plots. It is rendered at Monash University, Melbourne Australia. The participants are all students or lecturers in this university.

Four survey are randomly sent to 84 people by email, three of the survey consist of ten lineup questions, and the fourth survey has only four lineup questions. Only one lineup question appears in the survey twice, thus, we have 33 ($10\times3+4-1$) distinct questions in total. A total number of 22 people have participated. Five people evaluated two surveys. One people selected four plots for each lineup, this person's response is removed from the data. In summary, we have 218 effective evaluations from 21 people. 

Figure \ref{heterlu} is an example of the lineup used as a question in the survey.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/heter_lineup_example.png}}
\caption{An example question in the survey, 4 out of 6 people picked the real plot, the real plot is the first one.}
\label{heterlu}
\end{figure*}

The "real plot" and "null plot" data is simulated using the same specifications given in the next two sections respectively.

## Heteroskedasticity simulation

Linear model with heteroskedasticity is the model implied in the alternative hypothesis of the second experiment in this paper, where the constant variance assumption of the linear model is violated while all other conditions are met. By the definition given in @IE17, "The homoskedasticity states that the variance of the unobserved error, u, conditional on the explanatory variables, is constant. Homoskedasticity fails whenver the variance of the unobserved factors changes across different segments of the population, where the segments are determined by the different values of the explanatory variables." There are countless types of heteroskedasticity since the "change of the variance" could be related to "the explanatory variables" in various ways. It is not feasible to list out all kinds of heteroskedasticity by a single function. For simplicity, we will focus on one example of them, a linear correlation between the explanatory variable X and the standard deviation of the error term. Hence the relationship between the explanatory variable X and the variance of the error term will be quadratic. The results from this experiment though can be generalized to more complicated cases.  

The model structure is the same with the classic linear model:
$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$
with elements of the model generated by the following processes

- $X\sim U[-1,\ 1]$
To better present the heteroskedasticity in the data, a uniform distribution of X is used instead of normal distribution. The range is set to be small (from -1 to 1) in order to balance the data with weak heteroskedasticity appearing more frequently.
- $\beta_0 = 0$
Intercept is set to be zero. Because the residual plot but not data plot is used in this experiment. Therefore, the information contained in $\beta_0$ will be extracted by the linear regression we fit to the data.
- $\beta_1\sim U[0.5,\ 1]$
$\beta_1$ has little impact in this case as well so it is set to be uniformly generated from 0.5 to 1. 
- $\varepsilon\sim N(0,\ (aX+v)^2)$
The variance of the error term is a quadratic function of the explanatory variable which controls the magnitude of heteroskedasticity in the model.
- $a\sim U(-5,\ -0.05)\bigcup(0.05,\ 5)$
The parameter a here, following uniform distribution from -5 to 5 (excluding -0.05 to 0.05), is the correlation coefficient between X and the standard deviation. Larger a gives stronger heteroskedasticity. This range is wide enough for our purpose.
- $v \sim N(0,\ 1)$
This new error term is added to the variance of $\varepsilon$ so the relationship between the data can be more flexible.
- $ax+v-min(ax+v)$ when $min(ax+v)<0$ 
To keep the simulated standard deviation positive, and to keep the structure of the relationship between X and the residuals, the $min(ax+v)$ is substracted from $ax+v$ whenever the former is negative.
- $n\sim U[50,\ 500]$
The sample sizes are randomly generated from 50 to 500 to provide reasonable variations.

In general, the choice of the parameters is an empirical work. Primarily, we want the residual plots to show more variation; on the other hand, we need to limit the range of these parameters in order to keep the key features in the data.

```{r heter_plot, message=FALSE, cache=TRUE, fig.cap="Four examples of residual plots generated from linear model with heteroskedasticity"}
heter<-function(i){
  
  n = sample(50:500, 1)
  x <- runif(n, -1, 1)
  beta <- runif(1,0.5,1)
  a <- runif(1,0.05,4)*(-1)^(rbinom(1, 1, 0.5))
  sd <- a*x+rnorm(n, 0, 1)
  
  min <- min(sd)
  if (min<0){
    sd <- sd-min
  }
  
  y<-rnorm(n, beta*x, sd)
  df <- tibble(x, y)
  model<-lm(y ~ x, data=df)
  fit <- augment(model, df)
  heter_data <- fit %>% select(x, .std.resid)
  pic <- ggplot(heter_data, aes(x=x, y=.std.resid))+geom_point(alpha=0.4) + theme(aspect.ratio = 1)
  
}
set.seed(0518)
p1 <- heter(1)
p2 <- heter(2)
p3 <- heter(3)
p4 <- heter(4)
grid.arrange(p1,p2,p3,p4, nrow=2) 

```

## Null plot simulation

The null scenario in this experiment is the classic linear model. The model structure is the same as the heteroskedasticity one. When we simulate this data, we kept most of the parameters as the same with the alternative data and only changed the key feature of the error term. So the difference in this data set is:

- $\varepsilon\sim N(0,\ c)$

- $c=mean(ax+v)$

$c$ is a constant which equals to the mean of the $ax+v$. All other parameters in the null data are the same with the heteroskedasticity data.

## White test

To provide a reference level of how computer and human perform, a special case of White test is used in this experiment. Every data set simulated from this section has been tested by the White test. The procedure of the White test [@IE17] is:

- Estimate OLS model for the data, obtain residuals ($\hat{u}$) and the fitted values ($\hat{y}$). Computer the squared OLS residuals ($\hat{u}^2$) and the squared fitted values ($\hat{y}^2$).

- Run an auxiliary regression as $\hat{u}^2=\eta_0+\eta_1  \hat{y}+\eta_2 \hat{y}^2+error$, obtain the R-squared $R_{\hat{u}^2}^2$

- Calculate the LM statistic which follows $\chi_2^2$ distribution

- Conclude based on p-values given certain $\alpha$

## Computer model

In this experiment, a linear model is fit to the data firstly. Residuals from the fitted model are standardized and extracted. The residual plot is made of standardized residuals against X. The model is still the convnets, and all hyper-parameter in this model are exactly the same as the previous one.

15 epoches are done in this section. All 15 convnets models are saved. The training and validation metrics are shown in figure \ref{histheter}.

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/heter_history_plot.png}}
\caption{Training and validation metrics of heteroskedasticity vs. null model in our second experiment}
\label{histheter}
\end{figure*}

## Comparing results  

\begin{figure*}[h]
\centerline{\includegraphics[width=15cm]{figures/human_against_a.png}}
\caption{Proportion of correct answers for each lineup question against the simulated correlation "a" from human evaluation}
\label{humana}
\end{figure*}


```{r heteresults, echo=F, paged.print=FALSE, results='asis'}
tests <- c("Computer 2%", "Human 5%", "White-test 5%", "White-test 2%", "Human 2%")
rank <- c(1,2,2,3,4)
correct <- c(25, 17, 17, 16, 15)
accuracy <- c("92.59%","62.96%", "62.96%","59.26%","55.56%")
tibble(rank, tests, correct, accuracy) %>% kable(col.names = c("Rank","Tests", "No. of correct", "Accuracy"), caption = "Accuracy of testing the 27 data plots evaluated by human computer and the conventional white-test.")
```


```{r hetercompare, echo=F, fig.height=6, fig.width=8, fig.cap="P.values for each real plot given by human, computer and white-test respectively."}
library(reshape2)
survey <- read.csv("~/documents/github/etf5550/heter&linear/survey_ans_nonull.csv")
survey <- survey %>% mutate(wtpvalue=1-pchisq(wt.stat,2))
graphdata <- survey %>% select(form, question, pval, pcpvalue, wtpvalue) %>% 
  mutate(id=1:27)
colnames(graphdata) <- c("form", "question", "5% Human p.values",
                         "Computer prediction of Null", 
                         "5% White-test p.values", "ID")

graphdata <- graphdata %>% melt(id=c("form","question","ID"))

ggplot(graphdata, aes(x=ID, y=value, group=variable, color=variable))+
  geom_point()+
  geom_line()+
  facet_wrap(~variable, nrow = 3)+
  #xlab("Lineup Questions")
  scale_x_continuous(name="Lineup Questions", breaks=1:27, 
                   labels=c("1","2","3","4","5","6","7","8",
                            "9","10","11","12","13","14","15",
                            "16","17","18","19","20","21","22",
                            "23","24","25","26","27"))
```





















<!--chapter:end:03-chap3.Rmd-->

---
chapter: 4
knit: "bookdown::render_book"
---

# Conclusion and discussion

SUMMARISE RESULTS BRIEFLY
The convnet can be trained to perform similarly to human perception, when the structure in the residual plots is very specific. Performance on the linear vs no structure is comparable to the human subjects results. Performance on detecting heteroskedasticity is also good. 

SUMMARISE OTHER FINDINGS BRIEFLY
Performance of the convnet exceeds the results obtained by a $t$-test. 

WHAT WOULD YOU DO DIFFERENTLY
- Training of the convnet requires many images. Time to train the model was long.

- Other types of structure: could we have many classes of structure? 

- How would you expand the experiment to build a model for general residual plot reading?  





































<!--chapter:end:04-chap4.Rmd-->

