---
title: "Human vs. Computer"
author: "Shuofan Zhang"
date: "5/28/2018"
fontsize: 10pt
output: 
  beamer_presentation:
    fig_height: 5
    fig_width: 8
    highlight: tango
    theme: metropolis
header-includes:
  - \usepackage{MonashBlue}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE, dev.args=list(bg=grey(0.9), pointsize=11))
```

## Reminder of the first presentation
### Our goal
**Teach the computer to read residual plots**

A major component used to diagnose model fits is a plot of the residuals. Residual plots are used to assess:

- Gauss-Markov assumption
- Heteroskedasticity
- Clumps of outliers 
- ...

## Why plots? 

```{r saurus, fig.height=2, fig.width=3.5, fig.align="center"}
library(tidyverse)
library(datasauRus)
ggplot(datasaurus_dozen, aes(x = x, y = y, colour = dataset)) +
  geom_point(size=0.5, alpha=0.5) +
  theme_void() +
  theme(legend.position = "none") +
  #theme(aspect.ratio = 1) +
  facet_wrap( ~ dataset, ncol = 5)
```
$E(x)=54.3, E(y)= 47.8, sd(x) = 16.8, sd(y) = 26.9, r = -0.06$

## Visual inference

```{r visinf}
library(nullabor)
library(broom)
set.seed(0414)
x <- runif(100, -1, 1)
df <- tibble(x, y=2+3*x+x^2-0.5*x^3+rnorm(100)*0.3)
df_lm <- lm(y~x, data=df)
df_mod <- augment(df_lm, df)
ggplot(lineup(null_permute(".std.resid"), df_mod), aes(x=x, y=.std.resid)) + geom_point() + facet_wrap(~.sample)
```

## Convolutional neural network (convnets)

- Computer vision has advanced substantially
- If we can train a computer to read residual plots we can have it process a lot more data, than a human can manage.

## How convnets works: Diagram of convolution and max pooling

```{r diagconv, fig.retina=NULL, out.width='100%', echo=FALSE}
knitr::include_graphics("figures/diagconv.png")
```


## How convnets works

<!-- deep learning explain --> diagram

## Aside: Computers can't tell difference between blueberry muffins and chihuahuas 

```{r chihuahuas, fig.align='center', fig.cap="Computers can't tell difference between blueberry muffins and chihuahuas", fig.retina=NULL, out.width='100%', echo=FALSE}
knitr::include_graphics("figures/choc_chip_muffins.png")
```

## Our Experiments

- First experiment: Linear vs. Null

$H_0$: There are no relationships between the two variables. (Null)

$H_1$: There is linear relationship between the two variables where all Gauss-Markov assumptions are met. (Linear)

- Second experiment: Heteroskedasticity vs. Null

$H_0$: There is linear relationship between the two variables where all Gauss-Markov assumptions are met. (Null)

$H_1$: There is linear relationship between the two variables where the variance of the error term is not a constant while all other Gauss-Markov assumptions are met. (Heteroskedasticity)


## First Experiment: Linear vs. Null
### Amazon Mechanical Turk study

- Majumder et al (2013) conducted a large study to compare the performance of the lineup protocol, assessed by human evaluators, in comaprison to the classical test
- Experiment 2 examined $H_o: \beta_k=0$ vs $H_a: \beta_k\neq 0$ assessing the importance of including variable $k$ in the linear model, conducted with a $t$-test, and also lineup protocol
- 70 lineups of size 20 plots
- 351 evaluations by human subjects
-
- Trained deep learning model will be used to classify plots from this study. Accuracy will be compared with results by human subjects.

## Frist Experiment: Linear vs. Null
### Example lineup from Turk experiment 2  
<!--display-->
```{r turkexp, fig.align='center', fig.cap="63 of 65 identified the data plot in this lineup", fig.retina=NULL, out.width='100%', echo=FALSE}
knitr::include_graphics("figures/plot_turk2_300_350_12_3.png")
```

## First Experiment: Linear vs. Null
### Human experiment procedures (diagram)

![Procedure of computer model experiment](figures/diaghm.png)

## First experiment: Linear vs. Null
### Computer model procedures (diagram)

```{r diagpc, fig.align='center', fig.cap="Procedure of computer model experiment", fig.retina=NULL, out.width='100%', echo=FALSE}
knitr::include_graphics("figures/diagpc.png")
```

## First experiment: Linear vs. Null
### Computer model procedures

1. Simulate data (X, Y) from the null and the alternative models
2. Generate scatter plots of X and Y
3. Save scatter plots as $150\times150$ pixels images
4. Train a deep learning classifier to recognise the patterns from two groups
5. Test the model's performance on new data and compute the accuracy

## First Experiment: Linear vs. Null
### Data simulation {#sec:simulation}  

The linear model:

$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$

- $X \sim N[0,\ 1]$  

- $\beta_0 = 0$  

- $\beta_1\sim U[-10, -0.1] \bigcup [0.1, 10]$  

- $\varepsilon\sim N(0, \sigma^2) \ where\ \sigma \sim U[1,12]$  

- $n=U[50,500]$  

The null model:

- $\beta_1=0$

## First Experiment: Linear vs. Null
### Histogram of simulated parameters from linear model
![Histogram of simulated parameters for linear model](figures/histlinear.png)

## First Experiment: Linear vs. Null
### Histogram of simulated parameters from null model

```{r histnull, fig.align='center', fig.cap="Histogram of simulated parameters for null model", fig.retina=NULL, out.width='60%', echo=FALSE}
knitr::include_graphics("figures/histnull.png")
```


## First Experiment: Linear vs. Null
### Examples of scatter plots from the linear model

```{r levels}
library(gridExtra)
linear<-function(n){
   beta <- sample(c((-10:-0.1),(0.1:10)), 1)
  
  sigma <- sample(1:12 , 1)
  x<-rnorm(n, 0, 1)
  y<-rnorm(n, beta*x, sigma)
  
  tibble(x, y) %>% 
  ggplot(aes(x = x, y = y)) + 
    geom_point(alpha = 0.4) +
    theme(aspect.ratio = 1)
}
set.seed(0420)
n1 <- sample(c(100, 300), 1)
n2 <- sample(c(100, 300), 1)
n3 <- sample(c(100, 300), 1)
n4 <- sample(c(100, 300), 1)

p1 <- linear(n1)
p2 <- linear(n2)
p3 <- linear(n3)
p4 <- linear(n4)

grid.arrange(p1,p2,p3,p4,nrow=2, heights=c(1,1))
```

## First Experiment: Linear vs. Null
### Examples of scatter plots from the null model

```{r norela}
norela<-function(i){
  n <- sample(c(100, 300), 1)
  beta <- 0
  sigma <- sample(1:12 , 1)
  x<-rnorm(n, 0, 1)
  y<-rnorm(n, beta*x, sigma)
  pic <- tibble(x, y) %>% 
    ggplot(aes(x = x, y=y)) + 
    geom_point(alpha = 0.4) +
    theme(aspect.ratio = 1)
}
set.seed(0518)
p1 <- norela(1)
p2 <- norela(2)
p3 <- norela(3)
p4 <- norela(4)
grid.arrange(p1,p2,p3,p4, nrow=2)
```


## First Experiment: Linear vs. Null
### Convnets R code

<!-- adjust the fontsize of R code --> 
<!-- removed one layer for display-->

```{r modelstructure, echo=TRUE, tidy=TRUE, size=12, message=FALSE, warning=FALSE, results='none'}
library(keras)
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), 
                activation = "relu",
                input_shape = c(150, 150, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), 
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), 
                activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

## First Experiment: Linear vs. Null
### Convnets structure

![convnets model structure](figures/modelstruc.png)

## First Experiment: Linear vs. Null
### Training and validation metrics of convnets

```{r metrics1, fig.align='center', fig.cap="Training and validation metrics of convnets for the first experiment", fig.retina=NULL, out.width='70%', echo=FALSE}
knitr::include_graphics("figures/linear_history_plot.png")
```


## First Experiment: Linear vs. Null
### Convnets model selection

\begin{table}[ht]
\begin{center}
\begin{tabular}{|l|rrr|}\hline
Tests & Linear & Null & Overall \\\hline
4 epoch & 0.892 & 0.984 &  0.938 \\   
6 epoch & 0.889 & 0.986 & 0.937 \\
8 epoch & 0.896 & 0.981 & 0.939 \\
10 epoch & 0.904 & 0.971 & 0.938 \\
5\% $t$-test & 0.921 & 0.949 & 0.935\\\hline
\end{tabular}
\end{center}
\caption{Performance of four checkpoints from the {\em convnets} model, and the 5\% significant $t$-test, computed on the test set. Accuracy is reported for each class, and overall. There is a slight improvement as the number of epochs increases, with 10 epochs being reasonably close to the ideal $t$-test accuracy.}
\label{checkpoints}
\end{table}

The 8th model is chosen according to the overall accuracy on the test set.

## First Experiment: Linear vs. Null
### Computer model performance calculation 

The performance of the computer model for the Turk study data is tested in three steps:

- Re-generate the 70 "real plots" using the same data in Turk study (without null plots);

- Create a seperate test directory for the 70 "real plots" exclusively;

- The computer model's predicted accuracy over the 70 "real plots" are recorded as the model's performance.

## First Experiment: Linear vs. Null
### Human evaluation performance calculation 

The conclusion of human evaluation is obtained differently from the computer's. Because human evaluated "lineup", not only the "real plots". The performance is tested in five steps:

- Count total number of evaluations made by human for one lineup (N) and the number of correct answers for that lineup (k);

- Obtain N and k for all 70 lineup;

- Calculate p-value associated with each real plot using the formula introduced in section 2 of @MM13;

- Draw conclusion: reject the null when the calculated p-value is smaller than $\alpha$.

- The accuracy of the conclusions the 70 real plots is presenting for the human performance.

## First Experiment: Linear vs. Null
### Comparing results

```{r linearresults, echo=F, results='asis'}
library(knitr)
tests <- c("Human 5%", "Human 2%", "T-test 5%", "Computer 2%", "T-test 2%")
rank <- c(1,1,2,3,4)
correct <- c(47, 47, 43, 39, 39)
accuracy <- c(0.6714,0.6714, 0.6143, 0.5571, 0.5571)
tibble(rank, tests, correct, accuracy) %>% kable(col.names = c("Rank","Tests", "No. of correct", "Accuracy"), caption = "Accuracy of testing the 70 data plots evaluated by human computer and the conventional t-test.")
```

## First Experiment: Linear vs. Null
### Aside discussion

It is possible that the best classification strategy found by convnets is in fact the convetional t-test.

```{r ttdl, fig.align='center', fig.cap="The estimated power and overall accuracy of T-test and convnets for test set", fig.retina=NULL, out.width='80%', echo=FALSE}
knitr::include_graphics("figures/ttest_dl_graph.png")
```

## Second Experiment: Heteroskedasticity vs. Null
### Human experiment setup
The experiment is to evaluate the human ability of reading heteroskedasticity from residual plots. It is rendered at Monash University, Melbourne Australia. The participants are all students or lecturers in this university.
<!-- diagram or graph-->
Four survey are randomly sent to 84 people by email, three of the survey consist of ten lineup questions, and the fourth survey has only four lineup questions. Only one lineup question appears in the survey twice, thus, we have 33 ($10\times3+4-1$) distinct questions in total. A total number of 22 people have participated. Five people evaluated two surveys. One people selected four plots for each lineup, this person's response is removed from the data. In summary, we have 218 effective evaluations from 21 people. 

## Second Experiment: Heteroskedasticity vs. Null
### An example of questions used in the human experiment 

```{r heterlineup, fig.align='center', fig.cap="An example question in the survey, 4 out of 6 people picked the real plot.", fig.retina=NULL, out.width='80%', echo=FALSE}
knitr::include_graphics("figures/heter_lineup_example.png")
```

## Second Experiment: Heteroskedasticity vs. Null
### Heteroskedasticity simulation

The Heteroskedasticity model:

$$Y_i = \beta_0 + \beta_1 X_{i}  + \varepsilon_i, ~~i=1, \dots , n$$

- $X\sim U[-1,\ 1]$

- $\beta_0 = 0$

- $\beta_1\sim U[0.5,\ 1]$

- $\varepsilon\sim N(0,\ (aX+v)^2)$

- $a\sim U(-5,\ -0.05)\bigcup(0.05,\ 5)$

- $v \sim N(0,\ 1)$

- $ax+v-min(ax+v)$ when $min(ax+v)<0$ 

- $n\sim U[50,\ 500]$ 

The null model:

- $\varepsilon\sim N(0,\ c)$

- $c=mean(ax+v)$


## Second Experiment: Heteroskedasticity vs. Null
### White test

To provide a reference level of how computer and human perform, a special case of White test is used in this experiment. The procedure of the White test [@IE17] is:

- Estimate OLS model for the data, obtain residuals ($\hat{u}$) and the fitted values ($\hat{y}$). Computer the squared OLS residuals ($\hat{u}^2$) and the squared fitted values ($\hat{y}^2$).

- Run an auxiliary regression as $\hat{u}^2=\eta_0+\eta_1  \hat{y}+\eta_2 \hat{y}^2+error$, obtain the R-squared $R_{\hat{u}^2}^2$

- Calculate the LM statistic which follows $\chi_2^2$ distribution

- Conclude based on p-values given certain $\alpha$

## Second Experiment: Heteroskedasticity vs. Null
### Training and validation metrics of convnets

```{r metrics2, fig.align='center', fig.cap="Training and validation metrics of convnets for the second experiment", fig.retina=NULL, out.width='70%', echo=FALSE}
knitr::include_graphics("figures/human_against_a.png")
```

## Second Experiment: Heteroskedasticity vs. Null
### Convnets model selection

<!-- tables -->

## Second Experiment: Heteroskedasticity vs. Null
### Human performance in the experiment

```{r hheter, fig.align='center', fig.cap="Proportion of correct answers for each lineup question against the simulated correlation a from human evaluation", fig.retina=NULL, out.width='80%', echo=FALSE}
knitr::include_graphics("figures/heter_history_plot.png")
```

## Second Experiment: Heteroskedasticity vs. Null
### Comparing results

```{r heteresults, echo=F, paged.print=FALSE, results='asis'}
tests <- c("Computer 2%", "Human 5%", "White-test 5%", "White-test 2%", "Human 2%")
rank <- c(1,2,2,3,4)
correct <- c(25, 17, 17, 16, 15)
accuracy <- c("92.59%","62.96%", "62.96%","59.26%","55.56%")
tibble(rank, tests, correct, accuracy) %>% kable(col.names = c("Rank","Tests", "No. of correct", "Accuracy"), caption = "Accuracy of testing the 27 data plots evaluated by human computer and the conventional white-test.")
```

## Second Experiment: Heteroskedasticity vs. Null
### Comparing results for p.values

```{r heterp, fig.align='center', fig.cap="P.values for each real plot given by human, computer and white-test.", fig.retina=NULL, out.width='80%', echo=FALSE}
knitr::include_graphics("figures/heter_pvalues_3test.png")
```


## Materials

- The thesis, code and data is available on the github repository [https://github.com/shuofan18/ETF5550](https://github.com/shuofan18/ETF5550)
- Software used to conduct this research is R, Tensorflow, keras, tidyverse
